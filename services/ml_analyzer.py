"""
è½»é‡çº§æœºå™¨å­¦ä¹ åˆ†æå™¨ - ä½¿ç”¨ç®€å•çš„MLç®—æ³•è¿›è¡Œæ•°æ®åˆ†æ
"""
import numpy as np
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from collections import Counter, defaultdict

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from astrbot.api import logger

from ..config import PluginConfig
from ..exceptions import AnalysisError


class LightweightMLAnalyzer:
    """è½»é‡çº§æœºå™¨å­¦ä¹ åˆ†æå™¨ - é¿å…å¤§è§„æ¨¡æ•°æ®åˆ†æ"""
    
    def __init__(self, config: PluginConfig, db_manager: DatabaseManager):
        self.config = config
        self.db_manager = db_manager
        
        # è®¾ç½®åˆ†æé™åˆ¶ä»¥èŠ‚çœèµ„æº
        self.max_sample_size = 100  # æœ€å¤§æ ·æœ¬æ•°é‡
        self.max_features = 50      # æœ€å¤§ç‰¹å¾æ•°é‡
        self.analysis_cache = {}    # åˆ†æç»“æœç¼“å­˜
        self.cache_timeout = 3600   # ç¼“å­˜1å°æ—¶
        
        if not SKLEARN_AVAILABLE:
            logger.warning("scikit-learnæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€ç»Ÿè®¡åˆ†æ")
        
        logger.info("è½»é‡çº§MLåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    async def analyze_user_behavior_pattern(self, group_id: str, user_id: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"behavior_{group_id}_{user_id}"
            if self._check_cache(cache_key):
                return self.analysis_cache[cache_key]['data']
            
            # è·å–ç”¨æˆ·æœ€è¿‘æ¶ˆæ¯ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            messages = await self._get_user_messages(group_id, user_id, limit=self.max_sample_size)
            
            if not messages:
                return {}
            
            # åŸºç¡€ç»Ÿè®¡åˆ†æ
            pattern = {
                'message_count': len(messages),
                'avg_message_length': np.mean([len(msg['message']) for msg in messages]),
                'activity_hours': self._analyze_activity_hours(messages),
                'message_frequency': self._analyze_message_frequency(messages),
                'interaction_patterns': await self._analyze_interaction_patterns(group_id, user_id, messages)
            }
            
            # å¦‚æœæœ‰sklearnï¼Œè¿›è¡Œæ–‡æœ¬èšç±»
            if SKLEARN_AVAILABLE and len(messages) >= 5:
                pattern['topic_clusters'] = self._analyze_topic_clusters(messages)
            
            # ç¼“å­˜ç»“æœ
            self._cache_result(cache_key, pattern)
            
            return pattern
            
        except Exception as e:
            logger.error(f"åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼å¤±è´¥: {e}")
            raise AnalysisError(f"åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼å¤±è´¥: {str(e)}")

    async def _get_user_messages(self, group_id: str, user_id: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·æ¶ˆæ¯ï¼ˆé™åˆ¶æ•°é‡ï¼‰"""
        try:
            conn = await self.db_manager.get_group_connection(group_id)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT message, timestamp, sender_name
                FROM raw_messages 
                WHERE sender_id = ? AND timestamp > ?
                ORDER BY timestamp DESC 
                LIMIT ?
            ''', (user_id, time.time() - 86400 * 7, limit))  # æœ€è¿‘7å¤©
            
            messages = []
            for row in cursor.fetchall():
                messages.append({
                    'message': row,
                    'timestamp': row[1],
                    'sender_name': row
                })
            
            return messages
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
            return []

    def _analyze_activity_hours(self, messages: List[Dict[str, Any]]) -> Dict[str, float]:
        """åˆ†ææ´»åŠ¨æ—¶é—´æ¨¡å¼"""
        if not messages:
            return {}
        
        hour_counts = defaultdict(int)
        for msg in messages:
            hour = datetime.fromtimestamp(msg['timestamp']).hour
            hour_counts[hour] += 1
        
        total_messages = len(messages)
        hour_distribution = {
            str(hour): count / total_messages 
            for hour, count in hour_counts.items()
        }
        
        # ç¡®å®šæœ€æ´»è·ƒæ—¶æ®µ
        most_active_hour = max(hour_counts.items(), key=lambda x: x)[1]
        
        return {
            'distribution': hour_distribution,
            'most_active_hour': most_active_hour,
            'activity_variance': np.var(list(hour_counts.values()))
        }

    def _analyze_message_frequency(self, messages: List[Dict[str, Any]]) -> Dict[str, float]:
        """åˆ†ææ¶ˆæ¯é¢‘ç‡æ¨¡å¼"""
        if len(messages) < 2:
            return {}
        
        # è®¡ç®—æ¶ˆæ¯é—´éš”
        intervals = []
        sorted_messages = sorted(messages, key=lambda x: x['timestamp'])
        
        for i in range(1, len(sorted_messages)):
            interval = sorted_messages[i]['timestamp'] - sorted_messages[i-1]['timestamp']
            intervals.append(interval / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿ
        
        if not intervals:
            return {}
        
        return {
            'avg_interval_minutes': np.mean(intervals),
            'interval_std': np.std(intervals),
            'burst_tendency': len([x for x in intervals if x < 5]) / len(intervals)  # 5åˆ†é’Ÿå†…è¿ç»­æ¶ˆæ¯æ¯”ä¾‹
        }

    async def _analyze_interaction_patterns(self, group_id: str, user_id: str, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æäº’åŠ¨æ¨¡å¼"""
        try:
            # åˆ†æ@æ¶ˆæ¯å’Œå›å¤
            mention_count = len([msg for msg in messages if '@' in msg['message']])
            question_count = len([msg for msg in messages if '?' in msg['message'] or 'ï¼Ÿ' in msg['message']])
            
            # è·å–ç¤¾äº¤å…³ç³»å¼ºåº¦
            social_relations = await self.db_manager.load_social_graph(group_id)
            user_relations = [rel for rel in social_relations if rel['from_user'] == user_id or rel['to_user'] == user_id]
            
            return {
                'mention_ratio': mention_count / max(len(messages), 1),
                'question_ratio': question_count / max(len(messages), 1),
                'social_connections': len(user_relations),
                'avg_relation_strength': np.mean([rel['strength'] for rel in user_relations]) if user_relations else 0.0
            }
            
        except Exception as e:
            logger.error(f"åˆ†æäº’åŠ¨æ¨¡å¼å¤±è´¥: {e}")
            return {}

    def _analyze_topic_clusters(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨TF-IDFå’ŒK-meansè¿›è¡Œè¯é¢˜èšç±»"""
        if not SKLEARN_AVAILABLE or len(messages) < 3:
            return {}
        
        try:
            # æå–æ¶ˆæ¯æ–‡æœ¬
            texts = [msg['message'] for msg in messages if len(msg['message']) > 5]
            
            if len(texts) < 3:
                return {}
            
            # TF-IDFå‘é‡åŒ–ï¼ˆé™åˆ¶ç‰¹å¾æ•°é‡ï¼‰
            vectorizer = TfidfVectorizer(
                max_features=min(self.max_features, len(texts) * 2),
                stop_words=None,  # ä¸ä½¿ç”¨åœç”¨è¯ä»¥èŠ‚çœå†…å­˜
                ngram_range=(1, 1)  # åªä½¿ç”¨å•è¯
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # K-meansèšç±»ï¼ˆé™åˆ¶ç°‡æ•°é‡ï¼‰
            n_clusters = min(3, len(texts) // 2)
            if n_clusters < 2:
                return {}
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # åˆ†æèšç±»ç»“æœ
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                clusters[int(label)].append(texts[i][:50])  # é™åˆ¶æ–‡æœ¬é•¿åº¦
            
            # æå–å…³é”®è¯
            feature_names = vectorizer.get_feature_names_out()
            cluster_keywords = {}
            
            for i in range(n_clusters):
                center = kmeans.cluster_centers_[i]
                top_indices = center.argsort()[-5:][::-1]  # å‰5ä¸ªå…³é”®è¯
                cluster_keywords[i] = [feature_names[idx] for idx in top_indices]
            
            return {
                'n_clusters': n_clusters,
                'cluster_keywords': cluster_keywords,
                'cluster_sizes': {str(k): len(v) for k, v in clusters.items()}
            }
            
        except Exception as e:
            logger.error(f"è¯é¢˜èšç±»åˆ†æå¤±è´¥: {e}")
            return {}

    async def analyze_group_sentiment_trend(self, group_id: str) -> Dict[str, Any]:
        """åˆ†æç¾¤èŠæƒ…æ„Ÿè¶‹åŠ¿"""
        try:
            cache_key = f"sentiment_{group_id}"
            if self._check_cache(cache_key):
                return self.analysis_cache[cache_key]['data']
            
            # è·å–æœ€è¿‘æ¶ˆæ¯ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            recent_messages = await self._get_recent_group_messages(group_id, limit=self.max_sample_size)
            
            if not recent_messages:
                return {}
            
            # ç®€å•æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
            sentiment_trend = self._analyze_sentiment_keywords(recent_messages)
            
            # æ´»è·ƒåº¦åˆ†æ
            activity_trend = self._analyze_activity_trend(recent_messages)
            
            result = {
                'sentiment_trend': sentiment_trend,
                'activity_trend': activity_trend,
                'analysis_time': datetime.now().isoformat(),
                'sample_size': len(recent_messages)
            }
            
            self._cache_result(cache_key, result)
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æç¾¤èŠæƒ…æ„Ÿè¶‹åŠ¿å¤±è´¥: {e}")
            return {}

    async def _get_recent_group_messages(self, group_id: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–ç¾¤èŠæœ€è¿‘æ¶ˆæ¯"""
        try:
            conn = await self.db_manager.get_group_connection(group_id)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT message, timestamp, sender_id
                FROM raw_messages 
                WHERE timestamp > ?
                ORDER BY timestamp DESC 
                LIMIT ?
            ''', (time.time() - 3600 * 6, limit))  # æœ€è¿‘6å°æ—¶
            
            messages = []
            for row in cursor.fetchall():
                messages.append({
                    'message': row,
                    'timestamp': row[1],
                    'sender_id': row
                })
            
            return messages
            
        except Exception as e:
            logger.error(f"è·å–ç¾¤èŠæœ€è¿‘æ¶ˆæ¯å¤±è´¥: {e}")
            return []

    def _analyze_sentiment_keywords(self, messages: List[Dict[str, Any]]) -> Dict[str, float]:
        """åŸºäºå…³é”®è¯çš„ç®€å•æƒ…æ„Ÿåˆ†æ"""
        positive_keywords = ['å“ˆå“ˆ', 'å¥½çš„', 'è°¢è°¢', 'èµ', 'æ£’', 'å¼€å¿ƒ', 'é«˜å…´', 'ğŸ˜Š', 'ğŸ‘', 'â¤ï¸']
        negative_keywords = ['ä¸è¡Œ', 'å·®', 'çƒ¦', 'æ— èŠ', 'ç”Ÿæ°”', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ’”']
        
        positive_count = 0
        negative_count = 0
        total_messages = len(messages)
        
        for msg in messages:
            text = msg['message'].lower()
            for keyword in positive_keywords:
                if keyword in text:
                    positive_count += 1
                    break
            for keyword in negative_keywords:
                if keyword in text:
                    negative_count += 1
                    break
        
        return {
            'positive_ratio': positive_count / max(total_messages, 1),
            'negative_ratio': negative_count / max(total_messages, 1),
            'neutral_ratio': (total_messages - positive_count - negative_count) / max(total_messages, 1)
        }

    def _analyze_activity_trend(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ´»è·ƒåº¦è¶‹åŠ¿"""
        if not messages:
            return {}
        
        # æŒ‰å°æ—¶åˆ†ç»„ç»Ÿè®¡
        hourly_counts = defaultdict(int)
        for msg in messages:
            hour = datetime.fromtimestamp(msg['timestamp']).hour
            hourly_counts[hour] += 1
        
        # è®¡ç®—è¶‹åŠ¿
        hours = sorted(hourly_counts.keys())
        counts = [hourly_counts[hour] for hour in hours]
        
        if len(counts) >= 3:
            # ç®€å•çº¿æ€§è¶‹åŠ¿è®¡ç®—
            x = np.array(range(len(counts)))
            y = np.array(counts)
            trend_slope = np.polyfit(x, y, 1)
        else:
            trend_slope = 0
        
        return {
            'hourly_activity': dict(hourly_counts),
            'trend_slope': float(trend_slope),
            'peak_hour': max(hourly_counts.items(), key=lambda x: x) if hourly_counts else None[1],
            'total_activity': sum(counts)
        }

    def _check_cache(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self.analysis_cache:
            return False
        
        cache_time = self.analysis_cache[cache_key]['timestamp']
        return time.time() - cache_time < self.cache_timeout

    def _cache_result(self, cache_key: str, data: Dict[str, Any]):
        """ç¼“å­˜åˆ†æç»“æœ"""
        self.analysis_cache[cache_key] = {
            'data': data,
            'timestamp': time.time()
        }
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        current_time = time.time()
        expired_keys = [
            key for key, value in self.analysis_cache.items()
            if current_time - value['timestamp'] > self.cache_timeout
        ]
        
        for key in expired_keys:
            del self.analysis_cache[key]

    async def get_analysis_summary(self, group_id: str) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦"""
        try:
            # è·å–ç¾¤ç»Ÿè®¡
            group_stats = await self.db_manager.get_group_statistics(group_id)
            
            # è·å–æƒ…æ„Ÿè¶‹åŠ¿
            sentiment_trend = await self.analyze_group_sentiment_trend(group_id)
            
            # è·å–æœ€æ´»è·ƒç”¨æˆ·
            active_users = await self._get_most_active_users(group_id, limit=5)
            
            return {
                'group_statistics': group_stats,
                'sentiment_analysis': sentiment_trend,
                'active_users': active_users,
                'analysis_capabilities': {
                    'sklearn_available': SKLEARN_AVAILABLE,
                    'max_sample_size': self.max_sample_size,
                    'cache_status': len(self.analysis_cache)
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–åˆ†ææ‘˜è¦å¤±è´¥: {e}")
            return {}

    async def _get_most_active_users(self, group_id: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–æœ€æ´»è·ƒç”¨æˆ·"""
        try:
            conn = await self.db_manager.get_group_connection(group_id)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT sender_id, sender_name, COUNT(*) as message_count
                FROM raw_messages 
                WHERE timestamp > ?
                GROUP BY sender_id
                ORDER BY message_count DESC
                LIMIT ?
            ''', (time.time() - 86400, limit))  # æœ€è¿‘24å°æ—¶
            
            users = []
            for row in cursor.fetchall():
                users.append({
                    'user_id': row,
                    'user_name': row[1],
                    'message_count': row
                })
            
            return users
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ´»è·ƒç”¨æˆ·å¤±è´¥: {e}")
            return []
