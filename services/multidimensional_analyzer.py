"""
å¤šç»´åº¦å­¦ä¹ å¼•æ“ - å…¨æ–¹ä½åˆ†æç”¨æˆ·ç‰¹å¾å’Œç¤¾äº¤å…³ç³»
"""
import re
import json
import time
from typing import Dict, List, Optional, Any, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter

import asyncio # ç¡®ä¿ asyncio å¯¼å…¥
import re
import json
import time
from typing import Dict, List, Optional, Any, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter

from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent

from ..config import PluginConfig
from ..exceptions import StyleAnalysisError
from ..core.llm_client import LLMClient # å¯¼å…¥è‡ªå®šä¹‰LLMClient
from .database_manager import DatabaseManager # å¯¼å…¥ DatabaseManager


@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒ"""
    qq_id: str
    qq_name: str
    nicknames: List[str] = None
    activity_pattern: Dict[str, Any] = None
    communication_style: Dict[str, float] = None
    social_connections: List[str] = None
    topic_preferences: Dict[str, float] = None
    emotional_tendency: Dict[str, float] = None
    
    def __post_init__(self):
        if self.nicknames is None:
            self.nicknames = []
        if self.activity_pattern is None:
            self.activity_pattern = {}
        if self.communication_style is None:
            self.communication_style = {}
        if self.social_connections is None:
            self.social_connections = []
        if self.topic_preferences is None:
            self.topic_preferences = {}
        if self.emotional_tendency is None:
            self.emotional_tendency = {}


@dataclass
class SocialRelation:
    """ç¤¾äº¤å…³ç³»"""
    from_user: str
    to_user: str
    relation_type: str  # mention, reply, frequent_interaction
    strength: float  # å…³ç³»å¼ºåº¦ 0-1
    frequency: int   # äº¤äº’é¢‘æ¬¡
    last_interaction: str


@dataclass
class ContextualPattern:
    """æƒ…å¢ƒæ¨¡å¼"""
    context_type: str  # time_based, topic_based, social_based
    pattern_name: str
    triggers: List[str]
    characteristics: Dict[str, Any]
    confidence: float


class MultidimensionalAnalyzer:
    """å¤šç»´åº¦åˆ†æå™¨"""
    
    def __init__(self, config: PluginConfig, db_manager: DatabaseManager, context=None): # æ·»åŠ  db_manager å‚æ•°
        self.config = config
        self.context = context
        self.db_manager: DatabaseManager = db_manager # ç›´æ¥ä¼ å…¥ DatabaseManager å®ä¾‹
        
        # åˆå§‹åŒ–è‡ªå®šä¹‰ LLM å®¢æˆ·ç«¯
        self.filter_llm_client: Optional[LLMClient] = None
        if config.filter_api_url and config.filter_api_key and config.filter_model_name:
            self.filter_llm_client = LLMClient(
                api_url=config.filter_api_url,
                api_key=config.filter_api_key,
                model_name=config.filter_model_name
            )
        else:
            logger.warning("ç­›é€‰æ¨¡å‹LLMé…ç½®ä¸å®Œæ•´ï¼Œå°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œæ¶ˆæ¯ç­›é€‰ã€‚")

        self.refine_llm_client: Optional[LLMClient] = None
        if config.refine_api_url and config.refine_api_key and config.refine_model_name:
            self.refine_llm_client = LLMClient(
                api_url=config.refine_api_url,
                api_key=config.refine_api_key,
                model_name=config.refine_model_name
            )
        else:
            logger.warning("æç‚¼æ¨¡å‹LLMé…ç½®ä¸å®Œæ•´ï¼Œå°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æã€‚")

        self.reinforce_llm_client: Optional[LLMClient] = None
        if config.reinforce_api_url and config.reinforce_api_key and config.reinforce_model_name:
            self.reinforce_llm_client = LLMClient(
                api_url=config.reinforce_api_url,
                api_key=config.reinforce_api_key,
                model_name=config.reinforce_model_name
            )
        else:
            logger.warning("å¼ºåŒ–æ¨¡å‹LLMé…ç½®ä¸å®Œæ•´ï¼Œå°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚")
        
        # ç”¨æˆ·ç”»åƒå­˜å‚¨
        self.user_profiles: Dict[str, UserProfile] = {}
        
        # ç¤¾äº¤å…³ç³»å›¾è°±
        self.social_graph: Dict[str, List[SocialRelation]] = defaultdict(list)
        
        # æ˜µç§°æ˜ å°„è¡¨
        self.nickname_mapping: Dict[str, str] = {}  # nickname -> qq_id
        
        # æƒ…å¢ƒæ¨¡å¼åº“
        self.contextual_patterns: List[ContextualPattern] = []
        
        # è¯é¢˜åˆ†ç±»å™¨
        self.topic_keywords = {
            'æ—¥å¸¸èŠå¤©': ['åƒé¥­', 'ç¡è§‰', 'ä¸Šç­', 'ä¸‹ç­', 'ä¼‘æ¯', 'å¿™'],
            'æ¸¸æˆå¨±ä¹': ['æ¸¸æˆ', 'ç”µå½±', 'éŸ³ä¹', 'å°è¯´', 'åŠ¨æ¼«', 'ç»¼è‰º'],
            'å­¦ä¹ å·¥ä½œ': ['å­¦ä¹ ', 'å·¥ä½œ', 'é¡¹ç›®', 'è€ƒè¯•', 'ä¼šè®®', 'ä»»åŠ¡'],
            'æƒ…æ„Ÿäº¤æµ': ['å¼€å¿ƒ', 'éš¾è¿‡', 'ç”Ÿæ°”', 'æ‹…å¿ƒ', 'å…´å¥‹', 'æ— èŠ'],
            'æŠ€æœ¯è®¨è®º': ['ä»£ç ', 'ç¨‹åº', 'ç®—æ³•', 'æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹'],
            'ç”Ÿæ´»åˆ†äº«': ['æ—…æ¸¸', 'ç¾é£Ÿ', 'è´­ç‰©', 'å¥èº«', 'å® ç‰©', 'å®¶åº­']
        }
        
        logger.info("å¤šç»´åº¦å­¦ä¹ å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def start(self):
        """æœåŠ¡å¯åŠ¨æ—¶åŠ è½½ç”¨æˆ·ç”»åƒå’Œç¤¾äº¤å…³ç³»"""
        # å‡è®¾æ¯ä¸ªç¾¤ç»„æœ‰ç‹¬ç«‹çš„ç”»åƒå’Œå…³ç³»ï¼Œè¿™é‡Œéœ€è¦ä¸€ä¸ª group_id
        # ä¸ºäº†ç®€åŒ–ï¼Œæš‚æ—¶å‡è®¾åŠ è½½ä¸€ä¸ªé»˜è®¤çš„æˆ–å…¨å±€çš„ç”»åƒå’Œå…³ç³»
        # å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å½“å‰å¤„ç†çš„ç¾¤ç»„IDæ¥åŠ è½½
        default_group_id = "global_profiles" # æˆ–è€…ä»é…ç½®ä¸­è·å–
        
        # åŠ è½½æ‰€æœ‰ç”¨æˆ·ç”»åƒ (è¿™é‡Œéœ€è¦ DatabaseManager æä¾› load_all_user_profiles æ–¹æ³•)
        # æš‚æ—¶åªåŠ è½½ä¸€ä¸ªç¤ºä¾‹ç”¨æˆ·
        # loaded_profile_data = await self.db_manager.load_user_profile(default_group_id, "example_qq_id")
        # if loaded_profile_data:
        #     self.user_profiles[loaded_profile_data['qq_id']] = UserProfile(**loaded_profile_data)
        #     logger.info(f"å·²ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒ: {loaded_profile_data['qq_id']}")
        
        # åŠ è½½æ‰€æœ‰ç¤¾äº¤å…³ç³» (è¿™é‡Œéœ€è¦ DatabaseManager æä¾› load_all_social_relations æ–¹æ³•)
        # loaded_social_graph = await self.db_manager.load_social_graph(default_group_id)
        # for relation_data in loaded_social_graph:
        #     relation = SocialRelation(**relation_data)
        #     self.social_graph[relation.from_user].append(relation)
        # logger.info(f"å·²ä»æ•°æ®åº“åŠ è½½ {len(loaded_social_graph)} æ¡ç¤¾äº¤å…³ç³»ã€‚")
        
        logger.info("å¤šç»´åº¦å­¦ä¹ å¼•æ“å¯åŠ¨ï¼Œå‡†å¤‡è¿›è¡Œåˆ†æã€‚")

    async def filter_message_with_llm(self, message_text: str, current_persona_description: str) -> bool:
        """
        ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œæ™ºèƒ½ç­›é€‰ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸å½“å‰äººæ ¼åŒ¹é…ã€ç‰¹å¾é²œæ˜ä¸”æœ‰å­¦ä¹ æ„ä¹‰ã€‚
        è¿”å› True è¡¨ç¤ºæ¶ˆæ¯é€šè¿‡ç­›é€‰ï¼ŒFalse è¡¨ç¤ºä¸é€šè¿‡ã€‚
        """
        if not self.filter_llm_client:
            logger.warning("ç­›é€‰æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡LLMæ¶ˆæ¯ç­›é€‰ã€‚")
            # å¦‚æœLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå¯ä»¥æ ¹æ®å…¶ä»–ç®€å•è§„åˆ™è¿›è¡Œç­›é€‰ï¼Œæˆ–è€…ç›´æ¥è¿”å›True/False
            # è¿™é‡Œæš‚æ—¶è¿”å›Trueï¼Œè¡¨ç¤ºä¸è¿›è¡ŒLLMç­›é€‰
            return True

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ¶ˆæ¯ç­›é€‰ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ä¸€æ¡æ¶ˆæ¯æ˜¯å¦å…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š
1. ä¸å½“å‰äººæ ¼çš„å¯¹è¯é£æ ¼å’Œå…´è¶£é«˜åº¦åŒ¹é…ã€‚
2. æ¶ˆæ¯å†…å®¹ç‰¹å¾é²œæ˜ï¼Œä¸å¹³æ·¡ï¼Œå…·æœ‰ä¸€å®šçš„ç‹¬ç‰¹æ€§æˆ–æ·±åº¦ã€‚
3. å¯¹å­¦ä¹ å½“å‰äººæ ¼çš„å¯¹è¯æ¨¡å¼å’ŒçŸ¥è¯†æœ‰ç§¯ææ„ä¹‰ã€‚

å½“å‰äººæ ¼æè¿°ï¼š
{current_persona_description}

å¾…ç­›é€‰æ¶ˆæ¯ï¼š
"{message_text}"

è¯·ä½ æ ¹æ®ä»¥ä¸Šæ ‡å‡†ï¼Œå¯¹è¿™æ¡æ¶ˆæ¯è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç»™å‡ºä¸€ä¸ª0åˆ°1ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚
0è¡¨ç¤ºå®Œå…¨ä¸ç¬¦åˆï¼Œ1è¡¨ç¤ºå®Œå…¨ç¬¦åˆã€‚
è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å€¼ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚
"""
        try:
            response = await self.filter_llm_client.chat_completion(prompt=prompt)
            if response and response.text():
                numbers = re.findall(r'0\.\d+|1\.0|0', response.text().strip())
                if numbers:
                    confidence = min(float(numbers[0]), 1.0)
                    logger.debug(f"æ¶ˆæ¯ç­›é€‰ç½®ä¿¡åº¦: {confidence} (é˜ˆå€¼: {self.config.confidence_threshold})")
                    return confidence >= self.config.confidence_threshold
            logger.warning(f"LLMç­›é€‰æ¨¡å‹æœªè¿”å›æœ‰æ•ˆç½®ä¿¡åº¦ï¼Œæ¶ˆæ¯é»˜è®¤ä¸é€šè¿‡ç­›é€‰ã€‚")
            return False
        except Exception as e:
            logger.error(f"LLMæ¶ˆæ¯ç­›é€‰å¤±è´¥: {e}")
            return False

    async def evaluate_message_quality_with_llm(self, message_text: str, current_persona_description: str) -> Dict[str, float]:
        """
        ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œå¤šç»´åº¦é‡åŒ–è¯„åˆ†ã€‚
        è¯„åˆ†ç»´åº¦åŒ…æ‹¬ï¼šå†…å®¹è´¨é‡ã€ç›¸å…³æ€§ã€æƒ…æ„Ÿç§¯ææ€§ã€äº’åŠ¨æ€§ã€å­¦ä¹ ä»·å€¼ã€‚
        è¿”å›ä¸€ä¸ªåŒ…å«å„ç»´åº¦è¯„åˆ†çš„å­—å…¸ã€‚
        """
        if not self.refine_llm_client: # ä½¿ç”¨ refine_llm_client è¿›è¡Œæ›´å¤æ‚çš„åˆ†æ
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMè¿›è¡Œå¤šç»´åº¦é‡åŒ–è¯„åˆ†ã€‚")
            return {
                "content_quality": 0.5,
                "relevance": 0.5,
                "emotional_positivity": 0.5,
                "interactivity": 0.5,
                "learning_value": 0.5
            }

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†å¯¹ä¸€æ¡æ¶ˆæ¯è¿›è¡Œå¤šç»´åº¦é‡åŒ–è¯„åˆ†ã€‚
è¯„åˆ†èŒƒå›´ä¸º0åˆ°1ï¼Œ0è¡¨ç¤ºéå¸¸ä½ï¼Œ1è¡¨ç¤ºéå¸¸é«˜ã€‚

å½“å‰äººæ ¼æè¿°ï¼š
{current_persona_description}

å¾…è¯„ä¼°æ¶ˆæ¯ï¼š
"{message_text}"

è¯·è¯„ä¼°ä»¥ä¸‹ç»´åº¦å¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "content_quality": 0.0-1.0,  // æ¶ˆæ¯çš„æ·±åº¦ã€ä¿¡æ¯é‡ã€åŸåˆ›æ€§ã€è¡¨è¾¾æ¸…æ™°åº¦
    "relevance": 0.0-1.0,        // ä¸å½“å‰å¯¹è¯ä¸»é¢˜æˆ–äººæ ¼çš„ç›¸å…³æ€§
    "emotional_positivity": 0.0-1.0, // æ¶ˆæ¯çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æç¨‹åº¦ï¼‰
    "interactivity": 0.0-1.0,    // æ¶ˆæ¯æ˜¯å¦å¼•å‘æˆ–å›åº”äº†äº’åŠ¨ï¼ˆå¦‚æé—®ã€å›åº”ã€@ä»–äººï¼‰
    "learning_value": 0.0-1.0    // æ¶ˆæ¯å¯¹æ¨¡å‹å­¦ä¹ å½“å‰äººæ ¼å¯¹è¯æ¨¡å¼å’ŒçŸ¥è¯†çš„æ½œåœ¨è´¡çŒ®
}}

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå¹¶ä¸”åªåŒ…å«JSONå¯¹è±¡ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚
"""
        try:
            response = await self.refine_llm_client.chat_completion(prompt=prompt)
            if response and response.text():
                try:
                    scores = json.loads(response.text().strip())
                    # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                    for key, value in scores.items():
                        scores[key] = max(0.0, min(float(value), 1.0))
                    logger.debug(f"æ¶ˆæ¯å¤šç»´åº¦è¯„åˆ†: {scores}")
                    return scores
                except json.JSONDecodeError:
                    logger.warning(f"LLMå¤šç»´åº¦è¯„åˆ†å“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„åˆ†ã€‚å“åº”å†…å®¹: {response.text()}")
                    return {
                        "content_quality": 0.5,
                        "relevance": 0.5,
                        "emotional_positivity": 0.5,
                        "interactivity": 0.5,
                        "learning_value": 0.5
                    }
            logger.warning(f"LLMå¤šç»´åº¦è¯„åˆ†æ¨¡å‹æœªè¿”å›æœ‰æ•ˆå“åº”ï¼Œè¿”å›é»˜è®¤è¯„åˆ†ã€‚")
            return {
                "content_quality": 0.5,
                "relevance": 0.5,
                "emotional_positivity": 0.5,
                "interactivity": 0.5,
                "learning_value": 0.5
            }
        except Exception as e:
            logger.error(f"LLMå¤šç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
            return {
                "content_quality": 0.5,
                "relevance": 0.5,
                "emotional_positivity": 0.5,
                "interactivity": 0.5,
                "learning_value": 0.5
            }

    async def analyze_message_context(self, event: AstrMessageEvent, message_text: str) -> Dict[str, Any]:
        """åˆ†ææ¶ˆæ¯çš„å¤šç»´åº¦ä¸Šä¸‹æ–‡"""
        try:
            sender_id = event.get_sender_id()
            sender_name = event.get_sender_name()
            group_id = event.get_group_id()
            
            # æ›´æ–°ç”¨æˆ·ç”»åƒ
            await self._update_user_profile(group_id, sender_id, sender_name, message_text, event) # ä¼ å…¥ group_id
            
            # åˆ†æç¤¾äº¤å…³ç³»
            social_context = await self._analyze_social_context(event, message_text)
            
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ—¶é—´æ¨¡å¼
            temporal_context = await self._analyze_temporal_context(event)
            
            # åˆ†ææ²Ÿé€šé£æ ¼
            style_context = await self._analyze_communication_style(message_text)
            
            return {
                'user_profile': self.user_profiles.get(sender_id, {}).activity_pattern if sender_id in self.user_profiles else {},
                'social_context': social_context,
                'topic_context': topic_context,
                'emotional_context': emotional_context,
                'temporal_context': temporal_context,
                'style_context': style_context,
                'contextual_relevance': await self._calculate_contextual_relevance(
                    sender_id, message_text, event
                )
            }
            
        except Exception as e:
            logger.error(f"å¤šç»´åº¦ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {}

    async def _update_user_profile(self, group_id: str, qq_id: str, qq_name: str, message_text: str, event: AstrMessageEvent):
        """æ›´æ–°ç”¨æˆ·ç”»åƒå¹¶æŒä¹…åŒ–"""
        profile_data = await self.db_manager.load_user_profile(group_id, qq_id)
        if profile_data:
            profile = UserProfile(**profile_data)
        else:
            profile = UserProfile(qq_id=qq_id, qq_name=qq_name)
        
        # æ›´æ–°æ´»åŠ¨æ¨¡å¼
        current_hour = datetime.now().hour
        if 'activity_hours' not in profile.activity_pattern:
            profile.activity_pattern['activity_hours'] = Counter()
        profile.activity_pattern['activity_hours'][current_hour] += 1
        
        # æ›´æ–°æ¶ˆæ¯é•¿åº¦åå¥½
        msg_length = len(message_text)
        if 'message_lengths' not in profile.activity_pattern:
            profile.activity_pattern['message_lengths'] = []
        profile.activity_pattern['message_lengths'].append(msg_length)
        
        # ä¿æŒæœ€è¿‘100æ¡æ¶ˆæ¯çš„é•¿åº¦è®°å½•
        if len(profile.activity_pattern['message_lengths']) > 100:
            profile.activity_pattern['message_lengths'] = profile.activity_pattern['message_lengths'][-100:]
        
        # æ›´æ–°è¯é¢˜åå¥½
        topics = await self._extract_topics(message_text)
        for topic in topics:
            if topic not in profile.topic_preferences:
                profile.topic_preferences[topic] = 0
            profile.topic_preferences[topic] += 1
        
        # æ›´æ–°æ²Ÿé€šé£æ ¼
        style_features = await self._extract_style_features(message_text)
        for feature, value in style_features.items():
            if feature not in profile.communication_style:
                profile.communication_style[feature] = []
            profile.communication_style[feature].append(value)
            
            # ä¿æŒæœ€è¿‘50ä¸ªç‰¹å¾å€¼
            if len(profile.communication_style[feature]) > 50:
                profile.communication_style[feature] = profile.communication_style[feature][-50:]
        
        self.user_profiles[qq_id] = profile # æ›´æ–°å†…å­˜ä¸­çš„ç”»åƒ
        await self.db_manager.save_user_profile(group_id, asdict(profile)) # æŒä¹…åŒ–åˆ°æ•°æ®åº“

    async def _analyze_social_context(self, event: AstrMessageEvent, message_text: str) -> Dict[str, Any]:
        """åˆ†æç¤¾äº¤å…³ç³»ä¸Šä¸‹æ–‡"""
        try:
            sender_id = event.get_sender_id()
            group_id = event.get_group_id()
            
            social_context = {
                'mentions': [],
                'replies': [],
                'interaction_strength': {},
                'group_role': 'member'
            }
            
            # æå–@æ¶ˆæ¯
            mentions = self._extract_mentions(message_text)
            social_context['mentions'] = mentions
            
            # æ›´æ–°ç¤¾äº¤å…³ç³»
            for mentioned_user in mentions:
                await self._update_social_relation(
                    sender_id, mentioned_user, 'mention', group_id
                )
            
            # åˆ†æå›å¤å…³ç³»ï¼ˆå¦‚æœæ¡†æ¶æ”¯æŒï¼‰
            if hasattr(event, 'get_reply_info') and event.get_reply_info():
                reply_info = event.get_reply_info()
                replied_user = reply_info.get('user_id')
                if replied_user:
                    social_context['replies'].append(replied_user)
                    await self._update_social_relation(
                        sender_id, replied_user, 'reply', group_id
                    )
            
            # è®¡ç®—ä¸ç¾¤å†…æˆå‘˜çš„äº¤äº’å¼ºåº¦
            if sender_id in self.social_graph:
                for relation in self.social_graph[sender_id]:
                    social_context['interaction_strength'][relation.to_user] = relation.strength
            
            # åˆ†æç¾¤å†…è§’è‰²ï¼ˆåŸºäºå‘è¨€é¢‘ç‡å’Œ@æ¬¡æ•°ï¼‰
            group_role = await self._analyze_group_role(sender_id, group_id)
            social_context['group_role'] = group_role
            
            return social_context
        
        except Exception as e:
            logger.warning(f"ç¤¾äº¤ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {}

    async def _analyze_topic_context(self, message_text: str) -> Dict[str, float]:
        """åˆ†æè¯é¢˜ä¸Šä¸‹æ–‡"""
        topic_scores = {}
        
        for topic, keywords in self.topic_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in message_text:
                    score += 1
            
            if score > 0:
                topic_scores[topic] = score / len(keywords)
        
        return topic_scores

    async def _analyze_emotional_context(self, message_text: str) -> Dict[str, float]:
        """ä½¿ç”¨LLMåˆ†ææƒ…æ„Ÿä¸Šä¸‹æ–‡"""
        if not self.refine_llm_client:
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ†ææƒ…æ„Ÿä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_emotional_analysis(message_text)

        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç§¯æã€æ¶ˆæã€ä¸­æ€§ã€ç–‘é—®ã€æƒŠè®¶äº”ç§æƒ…æ„Ÿçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰ã€‚

æ–‡æœ¬å†…å®¹ï¼š"{message_text}"

è¯·åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¾‹å¦‚ï¼š
{{
    "ç§¯æ": 0.8,
    "æ¶ˆæ": 0.1,
    "ä¸­æ€§": 0.1,
    "ç–‘é—®": 0.0,
    "æƒŠè®¶": 0.0
}}
"""
            try:
                response = await self.refine_llm_client.chat_completion(prompt=prompt)
                
                if response and response.text():
                    try:
                        emotion_scores = json.loads(response.text().strip())
                        # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                        for key, value in emotion_scores.items():
                            emotion_scores[key] = max(0.0, min(float(value), 1.0))
                        return emotion_scores
                    except json.JSONDecodeError:
                        logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–æƒ…æ„Ÿåˆ†æã€‚å“åº”å†…å®¹: {response.text()}")
                        return self._simple_emotional_analysis(message_text)
                return self._simple_emotional_analysis(message_text)
                
            except Exception as e:
                logger.warning(f"LLMæƒ…æ„Ÿåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
        except Exception as e:
            logger.warning(f"LLMæƒ…æ„Ÿåˆ†æå¤±è´¥ - 2ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")


    def _simple_emotional_analysis(self, message_text: str) -> Dict[str, float]:
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æï¼ˆå¤‡ç”¨ï¼‰"""
        emotions = {
            'ç§¯æ': ['å¼€å¿ƒ', 'é«˜å…´', 'å…´å¥‹', 'æ»¡æ„', 'å–œæ¬¢', 'çˆ±', 'å¥½æ£’', 'å¤ªå¥½äº†', 'å“ˆå“ˆ', 'ğŸ˜„', 'ğŸ˜Š', 'ğŸ‘'],
            'æ¶ˆæ': ['éš¾è¿‡', 'ç”Ÿæ°”', 'å¤±æœ›', 'æ— èŠ', 'çƒ¦', 'è®¨åŒ', 'ç³Ÿç³•', 'ä¸å¥½', 'ğŸ˜­', 'ğŸ˜¢', 'ğŸ˜¡'],
            'ä¸­æ€§': ['çŸ¥é“', 'æ˜ç™½', 'å¯ä»¥', 'å¥½çš„', 'å—¯', 'å“¦', 'è¿™æ ·', 'ç„¶å'],
            'ç–‘é—®': ['å—', 'å‘¢', 'ï¼Ÿ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ğŸ¤”'],
            'æƒŠè®¶': ['å“‡', 'å¤©å“ª', 'çœŸçš„', 'ä¸ä¼šå§', 'å¤ª', 'ç«Ÿç„¶', 'å±…ç„¶', 'ğŸ˜±', 'ğŸ˜¯']
        }
        
        emotion_scores = {}
        total_words = len(message_text)
        
        for emotion, keywords in emotions.items():
            count = 0
            for keyword in keywords:
                count += message_text.count(keyword)
            
            emotion_scores[emotion] = count / max(total_words, 1)
        
        return emotion_scores

    async def _analyze_temporal_context(self, event: AstrMessageEvent) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´ä¸Šä¸‹æ–‡"""
        now = datetime.now()
        
        time_context = {
            'hour': now.hour,
            'weekday': now.weekday(),
            'time_period': self._get_time_period(now.hour),
            'is_weekend': now.weekday() >= 5,
            'season': self._get_season(now.month)
        }
        
        return time_context

    async def _analyze_communication_style(self, message_text: str) -> Dict[str, float]:
        """åˆ†ææ²Ÿé€šé£æ ¼"""
        style_features = {
            'formal_level': self._calculate_formal_level(message_text),
            'enthusiasm_level': self._calculate_enthusiasm_level(message_text),
            'question_tendency': self._calculate_question_tendency(message_text),
            'emoji_usage': self._calculate_emoji_usage(message_text),
            'length_preference': len(message_text),
            'punctuation_style': self._calculate_punctuation_style(message_text)
        }
        
        return style_features

    async def _extract_topics(self, message_text: str) -> List[str]:
        """æå–æ¶ˆæ¯è¯é¢˜"""
        detected_topics = []
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if keyword in message_text:
                    detected_topics.append(topic)
                    break
        
        return detected_topics

    async def _extract_style_features(self, message_text: str) -> Dict[str, float]:
        """æå–é£æ ¼ç‰¹å¾"""
        return {
            'length': len(message_text),
            'punctuation_ratio': len([c for c in message_text if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š']) / max(len(message_text), 1),
            'emoji_count': len(re.findall(r'[ğŸ˜€-]', message_text)),
            'question_count': message_text.count('ï¼Ÿ') + message_text.count('?'),
            'exclamation_count': message_text.count('ï¼') + message_text.count('!')
        }

    def _extract_mentions(self, message_text: str) -> List[str]:
        """æå–@æ¶ˆæ¯"""
        # åŒ¹é…@ç”¨æˆ·æ¨¡å¼
        at_pattern = r'@(\w+|\d+)'
        matches = re.findall(at_pattern, message_text)
        
        # å°è¯•è§£ææ˜µç§°åˆ°QQå·çš„æ˜ å°„
        mentioned_users = []
        for match in matches:
            if match.isdigit():
                # ç›´æ¥@çš„QQå·
                mentioned_users.append(match)
            else:
                # @çš„æ˜µç§°ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„QQå·
                if match in self.nickname_mapping:
                    mentioned_users.append(self.nickname_mapping[match])
                else:
                    # è®°å½•æœªçŸ¥æ˜µç§°
                    mentioned_users.append(f"nickname:{match}")
        
        return mentioned_users

    async def _update_social_relation(self, from_user: str, to_user: str, relation_type: str, group_id: str):
        """æ›´æ–°ç¤¾äº¤å…³ç³»"""
        # æŸ¥æ‰¾ç°æœ‰å…³ç³»
        existing_relation = None
        for relation in self.social_graph[from_user]:
            if relation.to_user == to_user and relation.relation_type == relation_type:
                existing_relation = relation
                break
        
        if existing_relation:
            # æ›´æ–°ç°æœ‰å…³ç³»
            existing_relation.frequency += 1
            existing_relation.last_interaction = datetime.now().isoformat()
            existing_relation.strength = min(existing_relation.strength + 0.1, 1.0)
        else:
            # åˆ›å»ºæ–°å…³ç³»
            new_relation = SocialRelation(
                from_user=from_user,
                to_user=to_user,
                relation_type=relation_type,
                strength=0.1,
                frequency=1,
                last_interaction=datetime.now().isoformat()
            )
            self.social_graph[from_user].append(new_relation)
        
        # æŒä¹…åŒ–ç¤¾äº¤å…³ç³»
        await self.db_manager.save_social_relation(group_id, asdict(existing_relation if existing_relation else new_relation))

    async def _analyze_group_role(self, user_id: str, group_id: str) -> str:
        """åˆ†æç”¨æˆ·åœ¨ç¾¤å†…çš„è§’è‰²"""
        # è¿™é‡Œå¯ä»¥åŸºäºå‘è¨€é¢‘ç‡ã€è¢«@æ¬¡æ•°ç­‰åˆ¤æ–­ç”¨æˆ·è§’è‰²
        # ç®€åŒ–å®ç°
        if user_id in self.user_profiles:
            profile = self.user_profiles[user_id]
            mention_count = sum(1 for relations in self.social_graph.values() 
                              for relation in relations 
                              if relation.to_user == user_id and relation.relation_type == 'mention')
            
            if mention_count > 10:
                return 'active_member'
            elif mention_count > 5:
                return 'regular_member'
            else:
                return 'member'
        
        return 'member'

    async def _calculate_contextual_relevance(self, sender_id: str, message_text: str, event: AstrMessageEvent) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¾—åˆ†"""
        relevance_score = 0.0
        
        # åŸºäºç”¨æˆ·å†å²è¡Œä¸ºçš„ç›¸å…³æ€§
        if sender_id in self.user_profiles:
            profile = self.user_profiles[sender_id]
            
            # è¯é¢˜ä¸€è‡´æ€§
            current_topics = await self._extract_topics(message_text)
            for topic in current_topics:
                if topic in profile.topic_preferences:
                    relevance_score += 0.2
            
            # é£æ ¼ä¸€è‡´æ€§
            current_style = await self._extract_style_features(message_text)
            if 'length' in profile.communication_style:
                avg_length = sum(profile.communication_style['length'][-10:]) / min(10, len(profile.communication_style['length']))
                length_similarity = 1.0 - abs(current_style['length'] - avg_length) / max(avg_length, 1)
                relevance_score += length_similarity * 0.1
        
            # æ—¶é—´ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            current_hour = datetime.now().hour
            if sender_id in self.user_profiles:
                profile = self.user_profiles[sender_id]
                if 'activity_hours' in profile.activity_pattern:
                    hour_frequency = profile.activity_pattern['activity_hours'].get(current_hour, 0)
                    total_messages = sum(profile.activity_pattern['activity_hours'].values())
                    if total_messages > 0:
                        time_relevance = hour_frequency / total_messages
                        relevance_score += time_relevance * 0.2
        
        return min(relevance_score, 1.0)

    def _get_time_period(self, hour: int) -> str:
        """è·å–æ—¶é—´æ®µ"""
        if 6 <= hour < 12:
            return 'ä¸Šåˆ'
        elif 12 <= hour < 18:
            return 'ä¸‹åˆ'
        elif 18 <= hour < 22:
            return 'æ™šä¸Š'
        else:
            return 'æ·±å¤œ'

    def _get_season(self, month: int) -> str:
        """è·å–å­£èŠ‚"""
        if month in [1, 2, 12]:
            return 'å†¬å­£'
        elif month in [3, 4, 5]:
            return 'æ˜¥å­£'
        elif month in [6, 7, 8]:
            return 'å¤å­£'
        else:
            return 'ç§‹å­£'

    async def _calculate_formal_level(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æ­£å¼ç¨‹åº¦"""
        if not self.refine_llm_client:
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMè®¡ç®—æ­£å¼ç¨‹åº¦ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_formal_level(text)

        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æ­£å¼ç¨‹åº¦ï¼Œä»0-1è¯„åˆ†ï¼Œ0è¡¨ç¤ºéå¸¸éšæ„ï¼Œ1è¡¨ç¤ºéå¸¸æ­£å¼ã€‚

åˆ†æç»´åº¦ï¼š
- ç§°è°“ä½¿ç”¨ï¼ˆæ‚¨/ä½ ï¼‰
- è¯­è¨€é£æ ¼ï¼ˆä¹¦é¢è¯­/å£è¯­ï¼‰
- ç¤¼è²Œç”¨è¯­é¢‘ç‡
- å¥å¼ç»“æ„å¤æ‚åº¦
- ä¸“ä¸šæœ¯è¯­ä½¿ç”¨

æ–‡æœ¬å†…å®¹ï¼š"{text}"

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å€¼ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚
"""
            
            response = await self.refine_llm_client.chat_completion(prompt=prompt)
            
            if response and response.text():
                numbers = re.findall(r'0\.\d+|1\.0|0', response.text().strip())
                if numbers:
                    return min(float(numbers[0]), 1.0)
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"LLMæ­£å¼ç¨‹åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
            return self._simple_formal_level(text)

    def _simple_formal_level(self, text: str) -> float:
        """ç®€åŒ–çš„æ­£å¼ç¨‹åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        formal_indicators = ['æ‚¨', 'è¯·', 'è°¢è°¢æ‚¨', 'ä¸å¥½æ„æ€', 'æ‰“æ‰°äº†', 'æ•æˆ‘ç›´è¨€', 'è¯·é—®']
        informal_indicators = ['å“ˆå“ˆ', 'å˜¿', 'å•Š', 'å‘€', 'å“¦', 'å—¯å—¯', 'å“‡']
        
        formal_count = sum(text.count(word) for word in formal_indicators)
        informal_count = sum(text.count(word) for word in informal_indicators)
        
        total = formal_count + informal_count
        return formal_count / max(total, 1) if total > 0 else 0.5

    async def _calculate_enthusiasm_level(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—çƒ­æƒ…ç¨‹åº¦"""
        if not self.refine_llm_client:
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMè®¡ç®—çƒ­æƒ…ç¨‹åº¦ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_enthusiasm_level(text)

        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„çƒ­æƒ…ç¨‹åº¦ï¼Œä»0-1è¯„åˆ†ï¼Œ0è¡¨ç¤ºéå¸¸å†·æ·¡ï¼Œ1è¡¨ç¤ºéå¸¸çƒ­æƒ…ã€‚

åˆ†æç»´åº¦ï¼š
- æ„Ÿå¹å·ä½¿ç”¨é¢‘ç‡
- ç§¯ææƒ…æ„Ÿè¯æ±‡
- è¡¨æƒ…ç¬¦å·ä½¿ç”¨
- è¯­æ°”å¼ºçƒˆç¨‹åº¦
- äº’åŠ¨æ„æ„¿è¡¨è¾¾

æ–‡æœ¬å†…å®¹ï¼š"{text}"

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å€¼ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚
"""
            
            response = await self.refine_llm_client.chat_completion(prompt=prompt)
            
            if response and response.text():
                numbers = re.findall(r'0\.\d+|1\.0|0', response.text().strip())
                if numbers:
                    return min(float(numbers[0]), 1.0)
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"LLMçƒ­æƒ…ç¨‹åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
            return self._simple_enthusiasm_level(text)

    def _simple_enthusiasm_level(self, text: str) -> float:
        """ç®€åŒ–çš„çƒ­æƒ…ç¨‹åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        enthusiasm_indicators = ['ï¼', '!', 'å“ˆå“ˆ', 'å¤ªå¥½äº†', 'æ£’', 'èµ', 'ğŸ˜„', 'ğŸ˜Š', 'ğŸ‰', 'å‰å®³', 'awesome']
        count = sum(text.count(indicator) for indicator in enthusiasm_indicators)
        return min(count / max(len(text), 1) * 20, 1.0)

    async def _calculate_question_tendency(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æé—®å€¾å‘"""
        if not self.refine_llm_client:
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMè®¡ç®—æé—®å€¾å‘ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_question_tendency(text)

        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æé—®å€¾å‘ï¼Œä»0-1è¯„åˆ†ï¼Œ0è¡¨ç¤ºå®Œå…¨æ²¡æœ‰ç–‘é—®ï¼Œ1è¡¨ç¤ºå¼ºçƒˆçš„æ±‚çŸ¥æ¬²å’Œç–‘é—®ã€‚

åˆ†æç»´åº¦ï¼š
- ç–‘é—®å¥æ•°é‡
- æ±‚çŸ¥æ¬²è¡¨è¾¾
- ä¸ç¡®å®šæ€§è¡¨è¿°
- å¾æ±‚æ„è§çš„è¯­æ°”
- æ¢ç´¢æ€§è¯­è¨€

æ–‡æœ¬å†…å®¹ï¼š"{text}"

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å€¼ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚
"""
            
            response = await self.refine_llm_client.chat_completion(prompt=prompt)
            
            if response and response.text():
                numbers = re.findall(r'0\.\d+|1\.0|0', response.text().strip())
                if numbers:
                    return min(float(numbers[0]), 1.0)
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"LLMæé—®å€¾å‘è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
            return self._simple_question_tendency(text)

    def _simple_question_tendency(self, text: str) -> float:
        """ç®€åŒ–çš„æé—®å€¾å‘è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        question_indicators = ['ï¼Ÿ', '?', 'å—', 'å‘¢', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å¦‚ä½•']
        count = sum(text.count(indicator) for indicator in question_indicators)
        return min(count / max(len(text), 1) * 10, 1.0)

    def _calculate_emoji_usage(self, text: str) -> float:
        """è®¡ç®—è¡¨æƒ…ç¬¦å·ä½¿ç”¨ç¨‹åº¦"""
        emoji_count = len(re.findall(r'[ğŸ˜€-]', text))
        return min(emoji_count / max(len(text), 1) * 10, 1.0)

    def _calculate_punctuation_style(self, text: str) -> float:
        """è®¡ç®—æ ‡ç‚¹ç¬¦å·é£æ ¼"""
        punctuation_count = len([c for c in text if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''()ï¼ˆï¼‰'])
        return punctuation_count / max(len(text), 1)

    async def get_user_insights(self, qq_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿ"""
        if qq_id not in self.user_profiles:
            return {"error": "ç”¨æˆ·ä¸å­˜åœ¨"}
        
        profile = self.user_profiles[qq_id]
        
        # è®¡ç®—ï¿½ï¿½ï¿½è·ƒæ—¶æ®µ
        active_hours = []
        if 'activity_hours' in profile.activity_pattern:
            sorted_hours = sorted(profile.activity_pattern['activity_hours'].items(), 
                                key=lambda x: x[1], reverse=True) # ä¿®æ­£æ’åºé”®
            active_hours = [hour for hour, count in sorted_hours[:3]]
        
        # è®¡ç®—ä¸»è¦è¯é¢˜
        main_topics = sorted(profile.topic_preferences.items(), 
                           key=lambda x: x[1], reverse=True)[:3] # ä¿®æ­£æ’åºé”®
        
        # è®¡ç®—ç¤¾äº¤æ´»è·ƒåº¦
        social_activity = len(self.social_graph.get(qq_id, []))
        
        # ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦æ´å¯Ÿ
        deep_insights = await self._generate_deep_insights(profile)
        
        return {
            'user_id': qq_id,
            'user_name': profile.qq_name,
            'nicknames': profile.nicknames,
            'active_hours': active_hours,
            'main_topics': [topic for topic, count in main_topics],
            'social_activity': social_activity,
            'communication_style_summary': self._summarize_communication_style(profile),
            'activity_summary': self._summarize_activity_pattern(profile),
            'deep_insights': deep_insights,
            'personality_analysis': await self._analyze_personality_traits(profile),
            'social_behavior': await self._analyze_social_behavior(qq_id)
        }

    async def _generate_deep_insights(self, profile: UserProfile) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿ"""
        if not self.refine_llm_client:
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿã€‚")
            return {"error": "LLMæœåŠ¡ä¸å¯ç”¨"}

        try:
            # å‡†å¤‡ç”¨æˆ·æ•°æ®æ‘˜è¦
            user_data_summary = {
                'qq_name': profile.qq_name,
                'nicknames': profile.nicknames,
                'topic_preferences': dict(list(profile.topic_preferences.items())[:5]),
                'activity_pattern': {
                    'peak_hours': [k for k, v in sorted(
                        profile.activity_pattern.get('activity_hours', {}).items(),
                        key=lambda item: item[1], reverse=True
                    )[:3]],
                    'avg_message_length': sum(profile.activity_pattern.get('message_lengths', [])) / 
                                        max(len(profile.activity_pattern.get('message_lengths', [])), 1)
                },
                'social_connections': len(profile.social_connections)
            }
            
            prompt = f"""
è¯·åŸºäºä»¥ä¸‹ç”¨æˆ·æ•°æ®ï¼Œç”Ÿæˆæ·±åº¦çš„ç”¨æˆ·ç”»åƒæ´å¯Ÿã€‚ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š

ç”¨æˆ·æ•°æ®ï¼š
{json.dumps(user_data_summary, ensure_ascii=False, indent=2)}

è¯·åˆ†æä»¥ä¸‹ç»´åº¦å¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š
{{
    "personality_type": "ç”¨æˆ·æ€§æ ¼ç±»å‹(å¦‚ï¼šå¤–å‘å‹/å†…å‘å‹/æ··åˆå‹)",
    "communication_preference": "æ²Ÿé€šåå¥½æè¿°",
    "social_role": "åœ¨ç¾¤ä½“ä¸­çš„è§’è‰²å®šä½",
    "activity_pattern_analysis": "æ´»åŠ¨æ¨¡å¼åˆ†æ",
    "interest_alignment": "å…´è¶£é¢†åŸŸå½’ç±»",
    "learning_potential": "å­¦ä¹ ä»·å€¼è¯„ä¼°(0-1)",
    "interaction_style": "äº’åŠ¨é£æ ¼ç‰¹å¾",
    "content_contribution": "å†…å®¹è´¡çŒ®åº¦è¯„ä¼°"
}}

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
            
            response = await self.refine_llm_client.chat_completion(prompt=prompt)
            
            if response and response.text():
                try:
                    insights = json.loads(response.text().strip())
                    return insights
                except json.JSONDecodeError:
                    logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–åˆ†æã€‚å“åº”å†…å®¹: {response.text()}")
                    return {
                        "personality_type": "åˆ†æä¸­",
                        "communication_preference": "å¾…æ·±å…¥åˆ†æ",
                        "social_role": "ç¾¤ä½“æˆå‘˜",
                        "learning_potential": 0.7
                    }
            return {"error": "LLMæœªè¿”å›æœ‰æ•ˆå“åº”"}
                
        except Exception as e:
            logger.warning(f"æ·±åº¦æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            return {"error": "æ´å¯Ÿç”Ÿæˆå¤±è´¥"}

    async def _analyze_personality_traits(self, profile: UserProfile) -> Dict[str, float]:
        """åˆ†æç”¨æˆ·äººæ ¼ç‰¹è´¨"""
        if not self.refine_llm_client:
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ†æäººæ ¼ç‰¹è´¨ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_personality_analysis(profile)

        try:
            # è·å–æœ€è¿‘çš„æ²Ÿé€šé£æ ¼æ•°æ®
            recent_styles = {}
            for feature, values in profile.communication_style.items():
                if values:
                    recent_styles[feature] = sum(values[-10:]) / min(len(values), 10)
            
            prompt = f"""
åŸºäºç”¨æˆ·çš„æ²Ÿé€šé£æ ¼æ•°æ®ï¼Œåˆ†æå…¶äººæ ¼ç‰¹è´¨ã€‚è¯·è¿”å›JSONæ ¼å¼çš„äº”å¤§äººæ ¼ç‰¹è´¨è¯„åˆ†(0-1)ï¼š

æ²Ÿé€šé£æ ¼æ•°æ®ï¼š
{json.dumps(recent_styles, ensure_ascii=False, indent=2)}

è¯·è¿”å›ä»¥ä¸‹æ ¼å¼çš„JSONï¼š
{{
    "openness": 0.0-1.0,  // å¼€æ”¾æ€§
    "conscientiousness": 0.0-1.0,  // å°½è´£æ€§  
    "extraversion": 0.0-1.0,  // å¤–å‘æ€§
    "agreeableness": 0.0-1.0,  // å®œäººæ€§
    "neuroticism": 0.0-1.0  // ç¥ç»è´¨
}}
"""
            
            response = await self.refine_llm_client.chat_completion(prompt=prompt)
            
            if response and response.text():
                try:
                    traits = json.loads(response.text().strip())
                    return traits
                except json.JSONDecodeError:
                    logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–äººæ ¼åˆ†æã€‚å“åº”å†…å®¹: {response.text()}")
                    return self._simple_personality_analysis(profile)
            return self._simple_personality_analysis(profile)
                
        except Exception as e:
            logger.warning(f"äººæ ¼ç‰¹è´¨åˆ†æå¤±è´¥: {e}")
            return self._simple_personality_analysis(profile)

    def _simple_personality_analysis(self, profile: UserProfile) -> Dict[str, float]:
        """ç®€åŒ–çš„äººæ ¼åˆ†æï¼ˆå¤‡ç”¨ï¼‰"""
        # åŸºäºåŸºç¡€æ•°æ®çš„ç®€å•åˆ†æ
        style_data = profile.communication_style
        
        # å¤–å‘æ€§ï¼šåŸºäºæ¶ˆæ¯é¢‘ç‡å’Œé•¿åº¦
        extraversion = 0.5
        if 'length' in style_data and style_data['length']:
            avg_length = sum(style_data['length'][-20:]) / min(len(style_data['length']), 20)
            extraversion = min(avg_length / 100, 1.0)
        
        # å¼€æ”¾æ€§ï¼šåŸºäºè¯é¢˜å¤šæ ·æ€§
        openness = len(profile.topic_preferences) / 10 if profile.topic_preferences else 0.5
        
        return {
            "openness": min(openness, 1.0),
            "conscientiousness": 0.6,  # é»˜è®¤å€¼
            "extraversion": extraversion,
            "agreeableness": 0.7,  # é»˜è®¤å€¼
            "neuroticism": 0.3   # é»˜è®¤å€¼
        }

    async def _analyze_social_behavior(self, qq_id: str) -> Dict[str, Any]:
        """åˆ†æç¤¾äº¤è¡Œä¸ºæ¨¡å¼"""
        if qq_id not in self.social_graph:
            return {"interaction_count": 0, "relationship_strength": {}}
        
        relations = self.social_graph[qq_id]
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„ç¤¾äº¤è¡Œä¸º
        behavior_stats = {
            "mention_frequency": len([r for r in relations if r.relation_type == 'mention']),
            "reply_frequency": len([r for r in relations if r.relation_type == 'reply']),
            "total_interactions": len(relations),
            "avg_relationship_strength": sum(r.strength for r in relations) / max(len(relations), 1),
            "top_connections": [
                {"user": r.to_user, "strength": r.strength, "frequency": r.frequency}
                for r in sorted(relations, key=lambda x: x.strength, reverse=True)[:5]
            ]
        }
        
        return behavior_stats

    def _summarize_communication_style(self, profile: UserProfile) -> Dict[str, str]:
        """æ€»ç»“æ²Ÿé€šé£æ ¼"""
        style_summary = {}
        
        if 'length' in profile.communication_style and profile.communication_style['length']:
            avg_length = sum(profile.communication_style['length']) / len(profile.communication_style['length'])
            if avg_length > 50:
                style_summary['length_style'] = 'è¯¦ç»†å‹'
            elif avg_length > 20:
                style_summary['length_style'] = 'é€‚ä¸­å‹'
            else:
                style_summary['length_style'] = 'ç®€æ´å‹'
        
        return style_summary

    def _summarize_activity_pattern(self, profile: UserProfile) -> Dict[str, Any]:
        """æ€»ç»“æ´»åŠ¨æ¨¡å¼"""
        activity_summary = {}
        
        if 'activity_hours' in profile.activity_pattern:
            hours = profile.activity_pattern['activity_hours']
            if hours:
                peak_hour = max(hours.items(), key=lambda x: x[1])[0] # ä¿®æ­£ä¸ºè·å–é”®
                activity_summary['peak_hour'] = peak_hour
                activity_summary['peak_period'] = self._get_time_period(peak_hour)
        
        return activity_summary

    async def export_social_graph(self) -> Dict[str, Any]:
        """å¯¼å‡ºç¤¾äº¤å…³ç³»å›¾è°±"""
        graph_data = {
            'nodes': [],
            'edges': [],
            'statistics': {}
        }
        
        # å¯¼å‡ºèŠ‚ç‚¹ï¼ˆç”¨æˆ·ï¼‰
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç”¨æˆ·ç”»åƒï¼Œè€Œä¸æ˜¯åªï¿½ï¿½ï¿½å†…å­˜ä¸­è·å–
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶ä½¿ç”¨å†…å­˜ä¸­çš„ user_profilesï¼Œä½†å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½
        for qq_id, profile in self.user_profiles.items():
            graph_data['nodes'].append({
                'id': qq_id,
                'name': profile.qq_name,
                'nicknames': profile.nicknames,
                'activity_level': len(profile.activity_pattern.get('activity_hours', {}))
            })
        
        # å¯¼å‡ºè¾¹ï¼ˆå…³ç³»ï¼‰
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç¤¾äº¤å…³ç³»ï¼Œè€Œä¸æ˜¯åªä»å†…å­˜ä¸­è·å–
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶ä½¿ç”¨å†…å­˜ä¸­çš„ social_graphï¼Œä½†å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½
        for from_user, relations in self.social_graph.items():
            for relation in relations:
                graph_data['edges'].append({
                    'from': from_user,
                    'to': relation.to_user,
                    'type': relation.relation_type,
                    'strength': relation.strength,
                    'frequency': relation.frequency
                })
        
        # ç»Ÿè®¡ä¿¡æ¯
        graph_data['statistics'] = {
            'total_users': len(self.user_profiles),
            'total_relations': sum(len(relations) for relations in self.social_graph.values()),
            'nickname_mappings': len(self.nickname_mapping)
        }
        
        return graph_data
