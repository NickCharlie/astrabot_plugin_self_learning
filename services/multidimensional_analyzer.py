"""
å¤šç»´åº¦å­¦ä¹ å¼•æ“ - å…¨æ–¹ä½åˆ†æç”¨æˆ·ç‰¹å¾å’Œç¤¾äº¤å…³ç³» - ç”¨æˆ·ç”»åƒ
"""
import re
import json
import time
import asyncio # ç¡®ä¿ asyncio å¯¼å…¥
from typing import Dict, List, Optional, Any, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import emoji # å¯¼å…¥ emoji åº“

from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent

from ..config import PluginConfig
from ..exceptions import StyleAnalysisError
from ..core.llm_client import LLMClient # å¯¼å…¥è‡ªå®šä¹‰LLMClientï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
from ..core.framework_llm_adapter import FrameworkLLMAdapter # å¯¼å…¥æ¡†æ¶é€‚é…å™¨
from .database_manager import DatabaseManager # å¯¼å…¥ DatabaseManager
from ..utils.json_utils import safe_parse_llm_json


@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒ"""
    qq_id: str
    qq_name: str
    nicknames: List[str] = None
    activity_pattern: Dict[str, Any] = None
    communication_style: Dict[str, float] = None
    social_connections: List[str] = None
    topic_preferences: Dict[str, float] = None
    emotional_tendency: Dict[str, float] = None
    
    def __post_init__(self):
        if self.nicknames is None:
            self.nicknames = []
        if self.activity_pattern is None:
            self.activity_pattern = {}
        if self.communication_style is None:
            self.communication_style = {}
        if self.social_connections is None:
            self.social_connections = []
        if self.topic_preferences is None:
            self.topic_preferences = {}
        if self.emotional_tendency is None:
            self.emotional_tendency = {}


@dataclass
class SocialRelation:
    """ç¤¾äº¤å…³ç³»"""
    from_user: str
    to_user: str
    relation_type: str  # mention, reply, frequent_interaction
    strength: float  # å…³ç³»å¼ºåº¦ 0-1
    frequency: int   # äº¤äº’é¢‘æ¬¡
    last_interaction: str


@dataclass
class ContextualPattern:
    """æƒ…å¢ƒæ¨¡å¼"""
    context_type: str  # time_based, topic_based, social_based
    pattern_name: str
    triggers: List[str]
    characteristics: Dict[str, Any]
    confidence: float


class MultidimensionalAnalyzer:
    """å¤šç»´åº¦åˆ†æå™¨"""
    
    def __init__(self, config: PluginConfig, db_manager: DatabaseManager, context=None,
                 llm_adapter: Optional[FrameworkLLMAdapter] = None,
                 prompts: Any = None): # æ·»åŠ  prompts å‚æ•°
        self.config = config
        self.context = context
        self.db_manager: DatabaseManager = db_manager # ç›´æ¥ä¼ å…¥ DatabaseManager å®ä¾‹
        self.prompts = prompts # ä¿å­˜ prompts

        # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        self.llm_adapter = llm_adapter

        # æ£€æŸ¥é…ç½®å®Œæ•´æ€§
        if self.llm_adapter:
            if not self.llm_adapter.has_filter_provider():
                logger.warning("ç­›é€‰æ¨¡å‹Provideræœªé…ç½®ã€‚å°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œæ¶ˆæ¯ç­›é€‰ã€‚")
            if not self.llm_adapter.has_refine_provider():
                logger.warning("æç‚¼æ¨¡å‹Provideræœªé…ç½®ã€‚å°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æã€‚")
            if not self.llm_adapter.has_reinforce_provider():
                logger.warning("å¼ºåŒ–æ¨¡å‹Provideræœªé…ç½®ã€‚å°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚")
        else:
            logger.warning("æ¡†æ¶LLMé€‚é…å™¨æœªé…ç½®ã€‚å°†æ— æ³•ä½¿ç”¨LLMè¿›è¡Œé«˜çº§åˆ†æã€‚")
        
        # ç”¨æˆ·ç”»åƒå­˜å‚¨
        self.user_profiles: Dict[str, UserProfile] = {}
        
        # ç¤¾äº¤å…³ç³»å›¾è°±
        self.social_graph: Dict[str, List[SocialRelation]] = defaultdict(list)
        
        # æ˜µç§°æ˜ å°„è¡¨
        self.nickname_mapping: Dict[str, str] = {}  # nickname -> qq_id
        
        # æƒ…å¢ƒæ¨¡å¼åº“
        self.contextual_patterns: List[ContextualPattern] = []
        
        # è¯é¢˜åˆ†ç±»å™¨
        self.topic_keywords = {
            'æ—¥å¸¸èŠå¤©': ['åƒé¥­', 'ç¡è§‰', 'ä¸Šç­', 'ä¸‹ç­', 'ä¼‘æ¯', 'å¿™'],
            'æ¸¸æˆå¨±ä¹': ['æ¸¸æˆ', 'ç”µå½±', 'éŸ³ä¹', 'å°è¯´', 'åŠ¨æ¼«', 'ç»¼è‰º'],
            'å­¦ä¹ å·¥ä½œ': ['å­¦ä¹ ', 'å·¥ä½œ', 'é¡¹ç›®', 'è€ƒè¯•', 'ä¼šè®®', 'ä»»åŠ¡'],
            'æƒ…æ„Ÿäº¤æµ': ['å¼€å¿ƒ', 'éš¾è¿‡', 'ç”Ÿæ°”', 'æ‹…å¿ƒ', 'å…´å¥‹', 'æ— èŠ'],
            'æŠ€æœ¯è®¨è®º': ['ä»£ç ', 'ç¨‹åº', 'ç®—æ³•', 'æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹'],
            'ç”Ÿæ´»åˆ†äº«': ['æ—…æ¸¸', 'ç¾é£Ÿ', 'è´­ç‰©', 'å¥èº«', 'å® ç‰©', 'å®¶åº­']
        }
        
        logger.info("å¤šç»´åº¦å­¦ä¹ å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def start(self):
        """æœåŠ¡å¯åŠ¨æ—¶åŠ è½½ç”¨æˆ·ç”»åƒå’Œç¤¾äº¤å…³ç³»"""
        try:
            logger.info("å¤šç»´åº¦åˆ†æå™¨å¯åŠ¨ä¸­...")
            
            # åˆå§‹åŒ–ç”¨æˆ·ç”»åƒå­˜å‚¨
            self.user_profiles = {}
            self.social_graph = {}
            
            # ä»æ•°æ®åº“åŠ è½½å·²æœ‰çš„ç”¨æˆ·ç”»åƒæ•°æ®
            try:
                await self._load_user_profiles_from_db()
            except Exception as e:
                logger.warning(f"ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            
            # ä»æ•°æ®åº“åŠ è½½ç¤¾äº¤å…³ç³»æ•°æ®
            try:
                await self._load_social_relations_from_db()
            except Exception as e:
                logger.warning(f"ä»æ•°æ®åº“åŠ è½½ç¤¾äº¤å…³ç³»å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–åˆ†æç¼“å­˜
            self._analysis_cache = {}
            self._cache_timeout = 3600  # 1å°æ—¶ç¼“å­˜
            
            # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
            self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
            
            logger.info(f"å¤šç»´åº¦åˆ†æå™¨å¯åŠ¨å®Œæˆï¼Œå·²åŠ è½½ {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒï¼Œ{len(self.social_graph)} ä¸ªç¤¾äº¤å…³ç³»")
            
        except Exception as e:
            logger.error(f"å¤šç»´åº¦åˆ†æå™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    async def _load_user_profiles_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒ"""
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒç¾¤ç»„
            conn = await self.db_manager._get_messages_db_connection()
            cursor = await conn.cursor()
            
            # æŸ¥è¯¢æœ€è¿‘æ´»è·ƒçš„ç”¨æˆ·
            await cursor.execute('''
                SELECT DISTINCT group_id, sender_id, sender_name, COUNT(*) as msg_count
                FROM raw_messages 
                WHERE timestamp > ? 
                GROUP BY group_id, sender_id
                HAVING msg_count >= 5
                ORDER BY msg_count DESC
                LIMIT 500
            ''', (time.time() - 7 * 24 * 3600,))  # æœ€è¿‘7å¤©
            
            users = await cursor.fetchall()
            
            for group_id, sender_id, sender_name, msg_count in users:
                if group_id and sender_id:
                    user_key = f"{group_id}:{sender_id}"
                    self.user_profiles[user_key] = {
                        'user_id': sender_id,
                        'name': sender_name or f"ç”¨æˆ·{sender_id}",
                        'group_id': group_id,
                        'message_count': msg_count,
                        'topics': [],
                        'communication_style': {},
                        'last_activity': time.time(),
                        'created_at': time.time()
                    }
            
            await conn.close()
            logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒ")
            
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
    
    async def _load_social_relations_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½ç¤¾äº¤å…³ç³»"""
        try:
            # åˆå§‹åŒ–ç¤¾äº¤å›¾è°±
            self.social_graph = {}
            
            # åˆ†æç”¨æˆ·é—´çš„äº¤äº’å…³ç³»
            conn = await self.db_manager._get_messages_db_connection()
            cursor = await conn.cursor()
            
            # æŸ¥è¯¢ç”¨æˆ·åœ¨åŒä¸€ç¾¤ç»„ä¸­çš„äº¤äº’
            await cursor.execute('''
                SELECT group_id, sender_id, COUNT(*) as interaction_count
                FROM raw_messages 
                WHERE timestamp > ? AND group_id IS NOT NULL
                GROUP BY group_id, sender_id
                HAVING interaction_count >= 3
            ''', (time.time() - 7 * 24 * 3600,))
            
            interactions = await cursor.fetchall()
            
            # æ„å»ºåŸºç¡€ç¤¾äº¤å…³ç³»
            for group_id, sender_id, count in interactions:
                if sender_id not in self.social_graph:
                    self.social_graph[sender_id] = []
                
                # ä¸ºç®€åŒ–ï¼Œæš‚æ—¶è®°å½•ç”¨æˆ·åœ¨å„ç¾¤ç»„çš„æ´»è·ƒåº¦
                relation_info = {
                    'target_user': group_id,
                    'relation_type': 'group_member',
                    'strength': min(1.0, count / 100.0),  # åŸºäºæ¶ˆæ¯æ•°é‡è®¡ç®—å…³ç³»å¼ºåº¦
                    'last_interaction': time.time()
                }
                self.social_graph[sender_id].append(relation_info)
            
            await conn.close()
            logger.info(f"æ„å»ºäº† {len(self.social_graph)} ä¸ªç”¨æˆ·çš„ç¤¾äº¤å…³ç³»")
            
        except Exception as e:
            logger.error(f"åŠ è½½ç¤¾äº¤å…³ç³»å¤±è´¥: {e}")
    
    async def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜å’Œæ•°æ®"""
        try:
            while True:
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
                
                current_time = time.time()
                
                # æ¸…ç†åˆ†æç¼“å­˜
                if hasattr(self, '_analysis_cache'):
                    expired_keys = [
                        k for k, v in self._analysis_cache.items()
                        if current_time - v.get('timestamp', 0) > self._cache_timeout
                    ]
                    for key in expired_keys:
                        del self._analysis_cache[key]
                    
                    if expired_keys:
                        logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸçš„åˆ†æç¼“å­˜")
                
                # æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·æ´»åŠ¨è®°å½•
                cutoff_time = current_time - 30 * 24 * 3600  # 30å¤©å‰
                expired_users = [
                    k for k, v in self.user_profiles.items()
                    if v.get('last_activity', 0) < cutoff_time
                ]
                
                for user_key in expired_users:
                    del self.user_profiles[user_key]
                    
                if expired_users:
                    logger.info(f"æ¸…ç†äº† {len(expired_users)} ä¸ªè¿‡æœŸçš„ç”¨æˆ·ç”»åƒ")
                    
        except asyncio.CancelledError:
            logger.info("å®šæœŸæ¸…ç†ä»»åŠ¡å·²å–æ¶ˆ")
        except Exception as e:
            logger.error(f"å®šæœŸæ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")

    async def filter_message_with_llm(self, message_text: str, current_persona_description: str) -> bool:
        """
        ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œæ™ºèƒ½ç­›é€‰ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸å½“å‰äººæ ¼åŒ¹é…ã€ç‰¹å¾é²œæ˜ä¸”æœ‰å­¦ä¹ æ„ä¹‰ã€‚
        è¿”å› True è¡¨ç¤ºæ¶ˆæ¯é€šè¿‡ç­›é€‰ï¼ŒFalse è¡¨ç¤ºä¸é€šè¿‡ã€‚
        """
        # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        if self.llm_adapter and self.llm_adapter.has_filter_provider():
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_FILTER_MESSAGE_PROMPT.format(
                current_persona_description=current_persona_description,
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.filter_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
                if response:
                    # è§£æç½®ä¿¡åº¦
                    numbers = re.findall(r'0\.\d+|1\.0|0', response.strip())
                    if numbers:
                        confidence = min(float(numbers[0]), 1.0)
                        logger.debug(f"æ¶ˆæ¯ç­›é€‰ç½®ä¿¡åº¦: {confidence} (é˜ˆå€¼: {self.config.confidence_threshold})")
                        return confidence >= self.config.confidence_threshold
                logger.warning(f"æ¡†æ¶é€‚é…å™¨ç­›é€‰æœªè¿”å›æœ‰æ•ˆç½®ä¿¡åº¦ï¼Œæ¶ˆæ¯é»˜è®¤ä¸é€šè¿‡ç­›é€‰ã€‚")
                return False
            except Exception as e:
                logger.error(f"LLMæ¶ˆæ¯ç­›é€‰å¤±è´¥: {e}")
                return False
        else:
            logger.warning("ç­›é€‰æ¨¡å‹æœªé…ç½®ï¼Œè·³è¿‡LLMæ¶ˆæ¯ç­›é€‰ã€‚")
            return True

    async def evaluate_message_quality_with_llm(self, message_text: str, current_persona_description: str) -> Dict[str, float]:
        """
        ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œå¤šç»´åº¦é‡åŒ–è¯„åˆ†ã€‚
        è¯„åˆ†ç»´åº¦åŒ…æ‹¬ï¼šå†…å®¹è´¨é‡ã€ç›¸å…³æ€§ã€æƒ…æ„Ÿç§¯ææ€§ã€äº’åŠ¨æ€§ã€å­¦ä¹ ä»·å€¼ã€‚
        è¿”å›ä¸€ä¸ªåŒ…å«å„ç»´åº¦è¯„åˆ†çš„å­—å…¸ã€‚
        """
        default_scores = {
            "content_quality": 0.5,
            "relevance": 0.5,
            "emotional_positivity": 0.5,
            "interactivity": 0.5,
            "learning_value": 0.5
        }

        # ä¼˜å…ˆä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        if self.llm_adapter and self.llm_adapter.has_refine_provider():
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_EVALUATE_MESSAGE_QUALITY_PROMPT.format(
                current_persona_description=current_persona_description,
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.refine_chat_completion(prompt=prompt)
                if response:
                    scores = safe_parse_llm_json(response, fallback_result=default_scores)
                    
                    if scores and isinstance(scores, dict):
                        # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                        for key, value in scores.items():
                            scores[key] = max(0.0, min(float(value), 1.0))
                        logger.debug(f"æ¶ˆæ¯å¤šç»´åº¦è¯„åˆ†: {scores}")
                        return scores
                    else:
                        return default_scores
                logger.warning(f"LLMå¤šç»´åº¦è¯„åˆ†æ¨¡å‹æœªè¿”å›æœ‰æ•ˆå“åº”ï¼Œè¿”å›é»˜è®¤è¯„åˆ†ã€‚")
                return default_scores
            except Exception as e:
                logger.error(f"LLMå¤šç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
                return default_scores
        else:
            logger.warning("æç‚¼æ¨¡å‹æœªé…ç½®ï¼Œè¿”å›é»˜è®¤è¯„åˆ†ã€‚")
            return default_scores

    async def analyze_message_context(self, event: AstrMessageEvent, message_text: str) -> Dict[str, Any]:
        """åˆ†ææ¶ˆæ¯çš„å¤šç»´åº¦ä¸Šä¸‹æ–‡"""
        try:
            # æ£€æŸ¥eventæ˜¯å¦ä¸ºNone
            if event is None:
                logger.info("ä½¿ç”¨ç®€åŒ–åˆ†ææ–¹å¼ï¼ˆæ— eventå¯¹è±¡ï¼‰")
                return await self._analyze_message_context_without_event(message_text)
            
            sender_id = event.get_sender_id()
            sender_name = event.get_sender_name()
            group_id = event.get_group_id()
            
            # æ›´æ–°ç”¨æˆ·ç”»åƒ
            await self._update_user_profile(group_id, sender_id, sender_name, message_text, event) # ä¼ å…¥ group_id
            
            # åˆ†æç¤¾äº¤å…³ç³»
            social_context = await self._analyze_social_context(event, message_text)
            
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ—¶é—´æ¨¡å¼
            temporal_context = await self._analyze_temporal_context(event)
            
            # åˆ†ææ²Ÿé€šé£æ ¼
            style_context = await self._analyze_communication_style(message_text)
            
            return {
                'user_profile': self.user_profiles.get(sender_id, {}).activity_pattern if sender_id in self.user_profiles else {},
                'social_context': social_context,
                'topic_context': topic_context,
                'emotional_context': emotional_context,
                'temporal_context': temporal_context,
                'style_context': style_context,
                'contextual_relevance': await self._calculate_contextual_relevance(
                    sender_id, message_text, event
                )
            }
            
        except Exception as e:
            logger.error(f"å¤šç»´åº¦ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {}

    async def analyze_message_batch(self, 
                                   message_text: str,
                                   sender_id: str = '',
                                   sender_name: str = '',
                                   group_id: str = '',
                                   timestamp: float = None) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†ææ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå­¦ä¹ æµç¨‹ä¸­çš„æ‰¹é‡å¤„ç†ï¼‰
        
        Args:
            message_text: æ¶ˆæ¯æ–‡æœ¬
            sender_id: å‘é€è€…ID
            sender_name: å‘é€è€…åç§°
            group_id: ç¾¤ç»„ID
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            Dict[str, Any]: åˆ†æç»“æœ
        """
        try:
            logger.debug(f"æ‰¹é‡åˆ†ææ¶ˆæ¯: å‘é€è€…={sender_id}, ç¾¤ç»„={group_id}, æ¶ˆæ¯é•¿åº¦={len(message_text)}")
            
            # æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆå¦‚æœæœ‰è¶³å¤Ÿä¿¡æ¯ï¼‰
            if sender_id and group_id:
                await self._update_user_profile_batch(group_id, sender_id, sender_name, message_text, timestamp)
            
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ²Ÿé€šé£æ ¼
            style_context = await self._analyze_communication_style(message_text)
            
            # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
            contextual_relevance = await self._calculate_enhanced_relevance(
                message_text, sender_id, group_id, timestamp
            )
            
            # æ„å»ºç®€åŒ–çš„ç¤¾äº¤ä¸Šä¸‹æ–‡
            social_context = {}
            if sender_id and group_id:
                social_context = await self._get_user_social_context(group_id, sender_id)
            
            return {
                'user_profile': self.user_profiles.get(f"{group_id}:{sender_id}", {}) if sender_id and group_id else {},
                'social_context': social_context,
                'topic_context': topic_context,
                'emotional_context': emotional_context,
                'temporal_context': {'timestamp': timestamp or time.time()},
                'style_context': style_context,
                'contextual_relevance': contextual_relevance
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ¶ˆæ¯åˆ†æå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€åˆ†æç»“æœ
            return await self._analyze_message_context_without_event(message_text)

    async def _update_user_profile_batch(self, group_id: str, sender_id: str, sender_name: str, 
                                       message_text: str, timestamp: float = None):
        """æ‰¹é‡æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            user_key = f"{group_id}:{sender_id}"
            current_time = timestamp or time.time()
            
            if user_key not in self.user_profiles:
                self.user_profiles[user_key] = {
                    'user_id': sender_id,
                    'name': sender_name,
                    'group_id': group_id,
                    'message_count': 0,
                    'topics': [],
                    'communication_style': {},
                    'last_activity': current_time,
                    'created_at': current_time
                }
            
            profile = self.user_profiles[user_key]
            profile['message_count'] += 1
            profile['last_activity'] = current_time
            
            # æ›´æ–°æ²Ÿé€šé£æ ¼
            style = await self._analyze_communication_style(message_text)
            if style:
                profile['communication_style'].update(style)
                
        except Exception as e:
            logger.error(f"æ‰¹é‡æ›´æ–°ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")

    async def _calculate_enhanced_relevance(self, message_text: str, sender_id: str = '', 
                                          group_id: str = '', timestamp: float = None) -> float:
        """è®¡ç®—å¢å¼ºçš„ç›¸å…³æ€§å¾—åˆ†"""
        try:
            # åŸºç¡€ç›¸å…³æ€§
            base_relevance = await self._calculate_basic_relevance(message_text)
            
            # ç”¨æˆ·æ´»è·ƒåº¦åŠ æˆ
            user_bonus = 0.0
            if sender_id and group_id:
                user_key = f"{group_id}:{sender_id}"
                if user_key in self.user_profiles:
                    user_profile = self.user_profiles[user_key]
                    # æ´»è·ƒç”¨æˆ·çš„æ¶ˆæ¯è·å¾—æ›´é«˜æƒé‡
                    if user_profile.get('message_count', 0) > 10:
                        user_bonus = 0.1
                    
            return min(1.0, base_relevance + user_bonus)
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¢å¼ºç›¸å…³æ€§å¤±è´¥: {e}")
            return await self._calculate_basic_relevance(message_text)

    async def _get_user_social_context(self, group_id: str, sender_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ç¤¾äº¤ä¸Šä¸‹æ–‡"""
        try:
            user_key = f"{group_id}:{sender_id}"
            if user_key in self.user_profiles:
                profile = self.user_profiles[user_key]
                return {
                    'message_count': profile.get('message_count', 0),
                    'activity_level': 'high' if profile.get('message_count', 0) > 50 else 'low',
                    'last_activity': profile.get('last_activity', 0)
                }
            return {}
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ç¤¾äº¤ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {}

    async def _analyze_message_context_without_event(self, message_text: str) -> Dict[str, Any]:
        """åœ¨æ²¡æœ‰eventå¯¹è±¡æ—¶åˆ†ææ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ²Ÿé€šé£æ ¼
            style_context = await self._analyze_communication_style(message_text)
            
            # è®¡ç®—åŸºç¡€ç›¸å…³æ€§å¾—åˆ†
            contextual_relevance = await self._calculate_basic_relevance(message_text)
            
            return {
                'user_profile': {},
                'social_context': {},
                'topic_context': topic_context,
                'emotional_context': emotional_context,
                'temporal_context': {},
                'style_context': style_context,
                'contextual_relevance': contextual_relevance
            }
            
        except Exception as e:
            logger.error(f"ç®€åŒ–ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {
                'user_profile': {},
                'social_context': {},
                'topic_context': {},
                'emotional_context': {},
                'temporal_context': {},
                'style_context': {},
                'contextual_relevance': 0.5
            }

    async def _calculate_basic_relevance(self, message_text: str) -> float:
        """è®¡ç®—åŸºç¡€ç›¸å…³æ€§å¾—åˆ†"""
        try:
            # åŸºäºæ¶ˆæ¯é•¿åº¦å’Œå†…å®¹è´¨é‡çš„ç®€å•è¯„åˆ†
            message_length = len(message_text.strip())
            if message_length < 5:
                return 0.2
            elif message_length < 20:
                return 0.4
            elif message_length < 50:
                return 0.6
            else:
                return 0.8
        except Exception:
            return 0.5

    async def _update_user_profile(self, group_id: str, qq_id: str, qq_name: str, message_text: str, event: AstrMessageEvent):
        """æ›´æ–°ç”¨æˆ·ç”»åƒå¹¶æŒä¹…åŒ–"""
        profile_data = await self.db_manager.load_user_profile(group_id, qq_id)
        if profile_data:
            profile = UserProfile(**profile_data)
        else:
            profile = UserProfile(qq_id=qq_id, qq_name=qq_name)
        
        # æ›´æ–°æ´»åŠ¨æ¨¡å¼
        current_hour = datetime.now().hour
        if 'activity_hours' not in profile.activity_pattern:
            profile.activity_pattern['activity_hours'] = Counter()
        profile.activity_pattern['activity_hours'][current_hour] += 1
        
        # æ›´æ–°æ¶ˆæ¯é•¿åº¦åå¥½
        msg_length = len(message_text)
        if 'message_lengths' not in profile.activity_pattern:
            profile.activity_pattern['message_lengths'] = []
        profile.activity_pattern['message_lengths'].append(msg_length)
        
        # ä¿æŒæœ€è¿‘100æ¡æ¶ˆæ¯çš„é•¿åº¦è®°å½•
        if len(profile.activity_pattern['message_lengths']) > 100:
            profile.activity_pattern['message_lengths'] = profile.activity_pattern['message_lengths'][-100:]
        
        # æ›´æ–°è¯é¢˜åå¥½
        topics = await self._extract_topics(message_text)
        for topic in topics:
            if topic not in profile.topic_preferences:
                profile.topic_preferences[topic] = 0
            profile.topic_preferences[topic] += 1
        
        # æ›´æ–°æ²Ÿé€šé£æ ¼
        style_features = await self._extract_style_features(message_text)
        for feature, value in style_features.items():
            if feature not in profile.communication_style:
                profile.communication_style[feature] = []
            profile.communication_style[feature].append(value)
            
            # ä¿æŒæœ€è¿‘50ä¸ªç‰¹å¾å€¼
            if len(profile.communication_style[feature]) > 50:
                profile.communication_style[feature] = profile.communication_style[feature][-50:]
        
        self.user_profiles[qq_id] = profile # æ›´æ–°å†…å­˜ä¸­çš„ç”»åƒ
        await self.db_manager.save_user_profile(group_id, asdict(profile)) # æŒä¹…åŒ–åˆ°æ•°æ®åº“

    async def _analyze_social_context(self, event: AstrMessageEvent, message_text: str) -> Dict[str, Any]:
        """åˆ†æç¤¾äº¤å…³ç³»ä¸Šä¸‹æ–‡"""
        try:
            sender_id = event.get_sender_id()
            group_id = event.get_group_id()
            
            social_context = {
                'mentions': [],
                'replies': [],
                'interaction_strength': {},
                'group_role': 'member'
            }
            
            # æå–@æ¶ˆæ¯
            mentions = self._extract_mentions(message_text)
            social_context['mentions'] = mentions
            
            # æ›´æ–°ç¤¾äº¤å…³ç³»
            for mentioned_user in mentions:
                await self._update_social_relation(
                    sender_id, mentioned_user, 'mention', group_id
                )
            
            # åˆ†æå›å¤å…³ç³»ï¼ˆå¦‚æœæ¡†æ¶æ”¯æŒï¼‰
            if hasattr(event, 'get_reply_info') and event.get_reply_info():
                reply_info = event.get_reply_info()
                replied_user = reply_info.get('user_id')
                if replied_user:
                    social_context['replies'].append(replied_user)
                    await self._update_social_relation(
                        sender_id, replied_user, 'reply', group_id
                    )
            
            # è®¡ç®—ä¸ç¾¤å†…æˆå‘˜çš„äº¤äº’å¼ºåº¦
            if sender_id in self.social_graph:
                for relation in self.social_graph[sender_id]:
                    social_context['interaction_strength'][relation.to_user] = relation.strength
            
            # åˆ†æç¾¤å†…è§’è‰²ï¼ˆåŸºäºå‘è¨€é¢‘ç‡å’Œ@æ¬¡æ•°ï¼‰
            group_role = await self._analyze_group_role(sender_id, group_id)
            social_context['group_role'] = group_role
            
            return social_context
        
        except Exception as e:
            logger.warning(f"ç¤¾äº¤ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {}

    async def _analyze_topic_context(self, message_text: str) -> Dict[str, float]:
        """åˆ†æè¯é¢˜ä¸Šä¸‹æ–‡"""
        topic_scores = {}
        
        for topic, keywords in self.topic_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in message_text:
                    score += 1
            
            if score > 0:
                topic_scores[topic] = score / len(keywords)
        
        return topic_scores

    async def _analyze_emotional_context(self, message_text: str) -> Dict[str, float]:
        """ä½¿ç”¨LLMåˆ†ææƒ…æ„Ÿä¸Šä¸‹æ–‡"""
        # ä¼˜å…ˆä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        if self.llm_adapter and self.llm_adapter.has_refine_provider():
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_EMOTIONAL_CONTEXT_PROMPT.format(
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.refine_chat_completion(prompt=prompt)
                
                if response:
                    # ä½¿ç”¨å®‰å…¨çš„JSONè§£ææ–¹æ³•
                    emotion_scores = safe_parse_llm_json(
                        response, 
                        fallback_result=self._simple_emotional_analysis(message_text)
                    )
                    
                    if emotion_scores and isinstance(emotion_scores, dict):
                        # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                        for key, value in emotion_scores.items():
                            emotion_scores[key] = max(0.0, min(float(value), 1.0))
                        logger.debug(f"æƒ…æ„Ÿä¸Šä¸‹æ–‡åˆ†æç»“æœ: {emotion_scores}")
                        return emotion_scores
                    else:
                        logger.warning(f"LLMæƒ…æ„Ÿåˆ†æè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
                        return self._simple_emotional_analysis(message_text)
                else:
                    logger.warning(f"LLMæƒ…æ„Ÿåˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
                    return self._simple_emotional_analysis(message_text)
                    
            except Exception as e:
                logger.error(f"LLMæƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
                return self._simple_emotional_analysis(message_text)
        
        # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨è¿›è¡Œæƒ…æ„Ÿä¸Šä¸‹æ–‡åˆ†æ
        elif self.llm_adapter and self.llm_adapter.has_refine_provider():
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_EMOTIONAL_CONTEXT_PROMPT.format(
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.2
                )
                
                if response:
                    # ä½¿ç”¨å®‰å…¨çš„JSONè§£ææ–¹æ³•
                    emotion_scores = safe_parse_llm_json(
                        response.strip(), 
                        fallback_result=self._simple_emotional_analysis(message_text)
                    )
                    
                    if emotion_scores and isinstance(emotion_scores, dict):
                        # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                        for key, value in emotion_scores.items():
                            emotion_scores[key] = max(0.0, min(float(value), 1.0))
                        logger.debug(f"æƒ…æ„Ÿä¸Šä¸‹æ–‡åˆ†æç»“æœ: {emotion_scores}")
                        return emotion_scores
                    else:
                        logger.warning(f"LLMæƒ…æ„Ÿåˆ†æè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
                        return self._simple_emotional_analysis(message_text)
                else:
                    logger.warning(f"LLMæƒ…æ„Ÿåˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
                    return self._simple_emotional_analysis(message_text)
                    
            except Exception as e:
                logger.error(f"LLMæƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
                return self._simple_emotional_analysis(message_text)
        else:
            logger.warning("æç‚¼æ¨¡å‹æœªé…ç½®ï¼Œä½¿ç”¨ç®€åŒ–æƒ…æ„Ÿåˆ†æç®—æ³•ã€‚")
            return self._simple_emotional_analysis(message_text)
        
    def _simple_emotional_analysis(self, message_text: str) -> Dict[str, float]:
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æï¼ˆå¤‡ç”¨ï¼‰"""
        emotions = {
            'ç§¯æ': ['å¼€å¿ƒ', 'é«˜å…´', 'å…´å¥‹', 'æ»¡æ„', 'å–œæ¬¢', 'çˆ±', 'å¥½æ£’', 'å¤ªå¥½äº†', 'å“ˆå“ˆ', 'ğŸ˜„', 'ğŸ˜Š', 'ğŸ‘'],
            'æ¶ˆæ': ['éš¾è¿‡', 'ç”Ÿæ°”', 'å¤±æœ›', 'æ— èŠ', 'çƒ¦', 'è®¨åŒ', 'ç³Ÿç³•', 'ä¸å¥½', 'ğŸ˜­', 'ğŸ˜¢', 'ğŸ˜¡'],
            'ä¸­æ€§': ['çŸ¥é“', 'æ˜ç™½', 'å¯ä»¥', 'å¥½çš„', 'å—¯', 'å“¦', 'è¿™æ ·', 'ç„¶å'],
            'ç–‘é—®': ['å—', 'å‘¢', 'ï¼Ÿ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ğŸ¤”'],
            'æƒŠè®¶': ['å“‡', 'å¤©å“ª', 'çœŸçš„', 'ä¸ä¼šå§', 'å¤ª', 'ç«Ÿç„¶', 'å±…ç„¶', 'ğŸ˜±', 'ğŸ˜¯']
        }
        
        emotion_scores = {}
        # å°†æ¶ˆæ¯æ–‡æœ¬æŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·åˆ†å‰²æˆå•è¯ï¼Œå¹¶è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
        words = [word for word in re.split(r'\s+|[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', message_text) if word]
        total_words = len(words) # å·²ç»ä¿®æ”¹ä¸ºå•è¯æ€»æ•°
        
        for emotion, keywords in emotions.items():
            count = 0
            for keyword in keywords:
                # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨å•è¯åˆ—è¡¨ä¸­
                count += words.count(keyword)
            
            emotion_scores[emotion] = count / max(total_words, 1)
        
        return emotion_scores

    async def _analyze_temporal_context(self, event: AstrMessageEvent) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´ä¸Šä¸‹æ–‡"""
        now = datetime.now()
        
        time_context = {
            'hour': now.hour,
            'weekday': now.weekday(),
            'time_period': self._get_time_period(now.hour),
            'is_weekend': now.weekday() >= 5,
            'season': self._get_season(now.month)
        }
        
        return time_context

    async def _analyze_communication_style(self, message_text: str) -> Dict[str, float]:
        """åˆ†ææ²Ÿé€šé£æ ¼"""
        style_features = {
            'formal_level': self._calculate_formal_level(message_text),
            'enthusiasm_level': self._calculate_enthusiasm_level(message_text),
            'question_tendency': self._calculate_question_tendency(message_text),
            'emoji_usage': self._calculate_emoji_usage(message_text),
            'length_preference': len(message_text),
            'punctuation_style': self._calculate_punctuation_style(message_text)
        }
        
        return style_features

    async def _extract_topics(self, message_text: str) -> List[str]:
        """æå–æ¶ˆæ¯è¯é¢˜"""
        detected_topics = []
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if keyword in message_text:
                    detected_topics.append(topic)
                    break
        
        return detected_topics

    async def _extract_style_features(self, message_text: str) -> Dict[str, float]:
        """æå–é£æ ¼ç‰¹å¾"""
        return {
            'length': len(message_text),
            'punctuation_ratio': len([c for c in message_text if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š']) / max(len(message_text), 1),
            'emoji_count': emoji.emoji_count(message_text),
            'question_count': message_text.count('ï¼Ÿ') + message_text.count('?'),
            'exclamation_count': message_text.count('ï¼') + message_text.count('!')
        }

    def _extract_mentions(self, message_text: str) -> List[str]:
        """æå–@æ¶ˆæ¯"""
        # åŒ¹é…@ç”¨æˆ·æ¨¡å¼
        at_pattern = r'@(\w+|\d+)'
        matches = re.findall(at_pattern, message_text)
        
        # å°è¯•è§£ææ˜µç§°åˆ°QQå·çš„æ˜ å°„
        mentioned_users = []
        for match in matches:
            if match.isdigit():
                # ç›´æ¥@çš„QQå·
                mentioned_users.append(match)
            else:
                # @çš„æ˜µç§°ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„QQå·
                if match in self.nickname_mapping:
                    mentioned_users.append(self.nickname_mapping[match])
                else:
                    # è®°å½•æœªçŸ¥æ˜µç§°
                    mentioned_users.append(f"nickname:{match}")
        
        return mentioned_users

    async def _update_social_relation(self, from_user: str, to_user: str, relation_type: str, group_id: str):
        """æ›´æ–°ç¤¾äº¤å…³ç³»"""
        # æŸ¥æ‰¾ç°æœ‰å…³ç³»
        existing_relation = None
        for relation in self.social_graph[from_user]:
            if relation.to_user == to_user and relation.relation_type == relation_type:
                existing_relation = relation
                break
        
        if existing_relation:
            # æ›´æ–°ç°æœ‰å…³ç³»
            existing_relation.frequency += 1
            existing_relation.last_interaction = datetime.now().isoformat()
            existing_relation.strength = min(existing_relation.strength + 0.1, 1.0)
        else:
            # åˆ›å»ºæ–°å…³ç³»
            new_relation = SocialRelation(
                from_user=from_user,
                to_user=to_user,
                relation_type=relation_type,
                strength=0.1,
                frequency=1,
                last_interaction=datetime.now().isoformat()
            )
            self.social_graph[from_user].append(new_relation)
        
        # æŒä¹…åŒ–ç¤¾äº¤å…³ç³»
        await self.db_manager.save_social_relation(group_id, asdict(existing_relation if existing_relation else new_relation))

    async def _analyze_group_role(self, user_id: str, group_id: str) -> str:
        """åˆ†æç”¨æˆ·åœ¨ç¾¤å†…çš„è§’è‰²"""
        # è¿™é‡Œå¯ä»¥åŸºäºå‘è¨€é¢‘ç‡ã€è¢«@æ¬¡æ•°ç­‰åˆ¤æ–­ç”¨æˆ·è§’è‰²
        # ç®€åŒ–å®ç°
        if user_id in self.user_profiles:
            profile = self.user_profiles[user_id]
            mention_count = sum(1 for relations in self.social_graph.values() 
                              for relation in relations 
                              if relation.to_user == user_id and relation.relation_type == 'mention')
            
            if mention_count > 10:
                return 'active_member'
            elif mention_count > 5:
                return 'regular_member'
            else:
                return 'member'
        
        return 'member'

    async def _calculate_contextual_relevance(self, sender_id: str, message_text: str, event: AstrMessageEvent) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¾—åˆ†"""
        relevance_score = 0.0
        
        # åŸºäºç”¨æˆ·å†å²è¡Œä¸ºçš„ç›¸å…³æ€§
        if sender_id in self.user_profiles:
            profile = self.user_profiles[sender_id]
            
            # è¯é¢˜ä¸€è‡´æ€§
            current_topics = await self._extract_topics(message_text)
            for topic in current_topics:
                if topic in profile.topic_preferences:
                    relevance_score += 0.2
            
            # é£æ ¼ä¸€è‡´æ€§
            current_style = await self._extract_style_features(message_text)
            if 'length' in profile.communication_style:
                avg_length = sum(profile.communication_style['length'][-10:]) / min(10, len(profile.communication_style['length']))
                length_similarity = 1.0 - abs(current_style['length'] - avg_length) / max(avg_length, 1)
                relevance_score += length_similarity * 0.1
        
            # æ—¶é—´ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            current_hour = datetime.now().hour
            if sender_id in self.user_profiles:
                profile = self.user_profiles[sender_id]
                if 'activity_hours' in profile.activity_pattern:
                    hour_frequency = profile.activity_pattern['activity_hours'].get(current_hour, 0)
                    total_messages = sum(profile.activity_pattern['activity_hours'].values())
                    if total_messages > 0:
                        time_relevance = hour_frequency / total_messages
                        relevance_score += time_relevance * 0.2
        
        return min(relevance_score, 1.0)

    def _get_time_period(self, hour: int) -> str:
        """è·å–æ—¶é—´æ®µ"""
        if 6 <= hour < 12:
            return 'ä¸Šåˆ'
        elif 12 <= hour < 18:
            return 'ä¸‹åˆ'
        elif 18 <= hour < 22:
            return 'æ™šä¸Š'
        else:
            return 'æ·±å¤œ'

    def _get_season(self, month: int) -> str:
        """è·å–å­£èŠ‚"""
        if month in [1, 2, 12]:
            return 'å†¬å­£'
        elif month in [3, 4, 5]:
            return 'æ˜¥å­£'
        elif month in [6, 7, 8]:
            return 'å¤å­£'
        else:
            return 'ç§‹å­£'

    async def _call_llm_for_style_analysis(self, text: str, prompt_template: str, fallback_function: callable, analysis_name: str) -> float:
        """
        é€šç”¨çš„LLMé£æ ¼åˆ†æè¾…åŠ©å‡½æ•°ã€‚
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬ã€‚
            prompt_template: LLMæç¤ºæ¨¡æ¿ã€‚
            fallback_function: LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–æˆ–è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨çš„å¤‡ç”¨å‡½æ•°ã€‚
            analysis_name: åˆ†æåç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
        Returns:
            0-1ä¹‹é—´çš„è¯„åˆ†ã€‚
        """
        if not (hasattr(self.config, 'refine_api_url') and hasattr(self.config, 'refine_api_key')):
            logger.warning(f"æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMè®¡ç®—{analysis_name}ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return fallback_function(text)
            
        if not (self.config.refine_api_url and self.config.refine_api_key):
            logger.warning(f"æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯é…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•ä½¿ç”¨LLMè®¡ç®—{analysis_name}ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return fallback_function(text)

        try:
            prompt = prompt_template.format(text=text)
            if self.llm_adapter and self.llm_adapter.has_refine_provider():
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
            else:
                response = None
            
            if response and response.text():
                numbers = re.findall(r'0\.\d+|1\.0|0', response.text().strip())
                if numbers:
                    return min(float(numbers[0]), 1.0)
            
            return 0.5 # é»˜è®¤å€¼
            
        except Exception as e:
            logger.warning(f"LLM{analysis_name}è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
            return fallback_function(text)

    async def _calculate_formal_level(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æ­£å¼ç¨‹åº¦"""
        prompt_template = self.prompts.MULTIDIMENSIONAL_ANALYZER_FORMAL_LEVEL_PROMPT
        return await self._call_llm_for_style_analysis(text, prompt_template, self._simple_formal_level, "æ­£å¼ç¨‹åº¦")

    def _simple_formal_level(self, text: str) -> float:
        """ç®€åŒ–çš„æ­£å¼ç¨‹åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        formal_indicators = ['æ‚¨', 'è¯·', 'è°¢è°¢æ‚¨', 'ä¸å¥½æ„æ€', 'æ‰“æ‰°äº†', 'æ•æˆ‘ç›´è¨€', 'è¯·é—®']
        informal_indicators = ['å“ˆå“ˆ', 'å˜¿', 'å•Š', 'å‘€', 'å“¦', 'å—¯å—¯', 'å“‡']
        
        formal_count = sum(text.count(word) for word in formal_indicators)
        informal_count = sum(text.count(word) for word in informal_indicators)
        
        total = formal_count + informal_count
        return formal_count / max(total, 1) if total > 0 else 0.5

    async def _calculate_enthusiasm_level(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—çƒ­æƒ…ç¨‹åº¦"""
        prompt_template = self.prompts.MULTIDIMENSIONAL_ANALYZER_ENTHUSIASM_LEVEL_PROMPT
        return await self._call_llm_for_style_analysis(text, prompt_template, self._simple_enthusiasm_level, "çƒ­æƒ…ç¨‹åº¦")

    def _simple_enthusiasm_level(self, text: str) -> float:
        """ç®€åŒ–çš„çƒ­æƒ…ç¨‹åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        enthusiasm_indicators = ['ï¼', '!', 'å“ˆå“ˆ', 'å¤ªå¥½äº†', 'æ£’', 'èµ', 'ğŸ˜„', 'ğŸ˜Š', 'ğŸ‰', 'å‰å®³', 'awesome']
        count = sum(text.count(indicator) for indicator in enthusiasm_indicators)
        return min(count / max(len(text), 1) * 20, 1.0)

    async def _calculate_question_tendency(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æé—®å€¾å‘"""
        prompt_template = self.prompts.MULTIDIMENSIONAL_ANALYZER_QUESTION_TENDENCY_PROMPT
        return await self._call_llm_for_style_analysis(text, prompt_template, self._simple_question_tendency, "æé—®å€¾å‘")

    def _simple_question_tendency(self, text: str) -> float:
        """ç®€åŒ–çš„æé—®å€¾å‘è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        question_indicators = ['ï¼Ÿ', '?', 'å—', 'å‘¢', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å¦‚ä½•']
        count = sum(text.count(indicator) for indicator in question_indicators)
        return min(count / max(len(text), 1) * 10, 1.0)

    def _calculate_emoji_usage(self, text: str) -> float:
        """è®¡ç®—è¡¨æƒ…ç¬¦å·ä½¿ç”¨ç¨‹åº¦"""
        emoji_count = emoji.emoji_count(text)
        return min(emoji_count / max(len(text), 1) * 10, 1.0)

    def _calculate_punctuation_style(self, text: str) -> float:
        """è®¡ç®—æ ‡ç‚¹ç¬¦å·é£æ ¼"""
        punctuation_count = len([c for c in text if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''()ï¼ˆï¼‰'])
        return punctuation_count / max(len(text), 1)

    async def get_user_insights(self, qq_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿ"""
        if qq_id not in self.user_profiles:
            return {"error": "ç”¨æˆ·ä¸å­˜åœ¨"}
        
        profile = self.user_profiles[qq_id]
        
        # è®¡ç®—æ´»è·ƒæ—¶æ®µ
        active_hours = []
        if 'activity_hours' in profile.activity_pattern:
            sorted_hours = sorted(profile.activity_pattern['activity_hours'].items(), 
                                key=lambda x: x[1], reverse=True) # ä¿®æ­£æ’åºé”®
            active_hours = [hour for hour, count in sorted_hours[:3]]
        
        # è®¡ç®—ä¸»è¦è¯é¢˜
        main_topics = sorted(profile.topic_preferences.items(), 
                           key=lambda x: x[1], reverse=True)[:3] # ä¿®æ­£æ’åºé”®
        
        # è®¡ç®—ç¤¾äº¤æ´»è·ƒåº¦
        social_activity = len(self.social_graph.get(qq_id, []))
        
        # ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦æ´å¯Ÿ
        deep_insights = await self._generate_deep_insights(profile)
        
        return {
            'user_id': qq_id,
            'user_name': profile.qq_name,
            'nicknames': profile.nicknames,
            'active_hours': active_hours,
            'main_topics': [topic for topic, count in main_topics],
            'social_activity': social_activity,
            'communication_style_summary': self._summarize_communication_style(profile),
            'activity_summary': self._summarize_activity_pattern(profile),
            'deep_insights': deep_insights,
            'personality_analysis': await self._analyze_personality_traits(profile),
            'social_behavior': await self._analyze_social_behavior(qq_id)
        }

    async def _generate_deep_insights(self, profile: UserProfile) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿ"""
        if not (hasattr(self.config, 'refine_api_url') and hasattr(self.config, 'refine_api_key')):
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿã€‚")
            return {"error": "LLMæœåŠ¡ä¸å¯ç”¨"}
            
        if not (self.config.refine_api_url and self.config.refine_api_key):
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯é…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿã€‚")
            return {"error": "LLMæœåŠ¡ä¸å¯ç”¨"}

        try:
            # å‡†å¤‡ç”¨æˆ·æ•°æ®æ‘˜è¦
            user_data_summary = {
                'qq_name': profile.qq_name,
                'nicknames': profile.nicknames,
                'topic_preferences': dict(list(profile.topic_preferences.items())[:5]),
                'activity_pattern': {
                    'peak_hours': [k for k, v in sorted(
                        profile.activity_pattern.get('activity_hours', {}).items(),
                        key=lambda item: item[1], reverse=True
                    )[:3]],
                    'avg_message_length': sum(profile.activity_pattern.get('message_lengths', [])) / 
                                        max(len(profile.activity_pattern.get('message_lengths', [])), 1)
                },
                'social_connections': len(profile.social_connections)
            }
            
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_DEEP_INSIGHTS_PROMPT.format(
                user_data_summary=json.dumps(user_data_summary, ensure_ascii=False, indent=2)
            )
            
            if self.llm_adapter and self.llm_adapter.has_refine_provider():
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
            else:
                response = None
            
            if response and response.text():
                try:
                    insights = safe_parse_llm_json(response.text())
                    return insights
                except json.JSONDecodeError:
                    logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–åˆ†æã€‚å“åº”å†…å®¹: {response.text()}")
                    return {
                        "personality_type": "åˆ†æä¸­",
                        "communication_preference": "å¾…æ·±å…¥åˆ†æ",
                        "social_role": "ç¾¤ä½“æˆå‘˜",
                        "learning_potential": 0.7
                    }
            return {"error": "LLMæœªè¿”å›æœ‰æ•ˆå“åº”"}
                
        except Exception as e:
            logger.warning(f"æ·±åº¦æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            return {"error": "æ´å¯Ÿç”Ÿæˆå¤±è´¥"}

    async def _analyze_personality_traits(self, profile: UserProfile) -> Dict[str, float]:
        """åˆ†æç”¨æˆ·äººæ ¼ç‰¹è´¨"""
        if not (hasattr(self.config, 'refine_api_url') and hasattr(self.config, 'refine_api_key')):
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ†æäººæ ¼ç‰¹è´¨ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_personality_analysis(profile)
            
        if not (self.config.refine_api_url and self.config.refine_api_key):
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯é…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ†æäººæ ¼ç‰¹è´¨ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_personality_analysis(profile)

        try:
            # è·å–æœ€è¿‘çš„æ²Ÿé€šé£æ ¼æ•°æ®
            recent_styles = {}
            for feature, values in profile.communication_style.items():
                if values:
                    recent_styles[feature] = sum(values[-10:]) / min(len(values), 10)
            
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_PERSONALITY_TRAITS_PROMPT.format(
                communication_style_data=json.dumps(recent_styles, ensure_ascii=False, indent=2)
            )
            
            if self.llm_adapter and self.llm_adapter.has_refine_provider():
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
            else:
                response = None
            
            if response and response.text():
                try:
                    traits = safe_parse_llm_json(response.text())
                    return traits
                except json.JSONDecodeError:
                    logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–äººæ ¼åˆ†æã€‚å“åº”å†…å®¹: {response.text()}")
                    return self._simple_personality_analysis(profile)
            return self._simple_personality_analysis(profile)
                
        except Exception as e:
            logger.warning(f"äººæ ¼ç‰¹è´¨åˆ†æå¤±è´¥: {e}")
            return self._simple_personality_analysis(profile)

    def _simple_personality_analysis(self, profile: UserProfile) -> Dict[str, float]:
        """ç®€åŒ–çš„äººæ ¼åˆ†æï¼ˆå¤‡ç”¨ï¼‰"""
        # åŸºäºåŸºç¡€æ•°æ®çš„ç®€å•åˆ†æ
        style_data = profile.communication_style
        
        # å¤–å‘æ€§ï¼šåŸºäºæ¶ˆæ¯é¢‘ç‡å’Œé•¿åº¦
        extraversion = 0.5
        if 'length' in style_data and style_data['length']:
            avg_length = sum(style_data['length'][-20:]) / min(len(style_data['length']), 20)
            extraversion = min(avg_length / 100, 1.0)
        
        # å¼€æ”¾æ€§ï¼šåŸºäºè¯é¢˜å¤šæ ·æ€§
        openness = len(profile.topic_preferences) / 10 if profile.topic_preferences else 0.5
        
        return {
            "openness": min(openness, 1.0),
            "conscientiousness": 0.6,  # é»˜è®¤å€¼
            "extraversion": extraversion,
            "agreeableness": 0.7,  # é»˜è®¤å€¼
            "neuroticism": 0.3   # é»˜è®¤å€¼
        }

    async def _analyze_social_behavior(self, qq_id: str) -> Dict[str, Any]:
        """åˆ†æç¤¾äº¤è¡Œä¸ºæ¨¡å¼"""
        if qq_id not in self.social_graph:
            return {"interaction_count": 0, "relationship_strength": {}}
        
        relations = self.social_graph[qq_id]
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„ç¤¾äº¤è¡Œä¸º
        behavior_stats = {
            "mention_frequency": len([r for r in relations if r.relation_type == 'mention']),
            "reply_frequency": len([r for r in relations if r.relation_type == 'reply']),
            "total_interactions": len(relations),
            "avg_relationship_strength": sum(r.strength for r in relations) / max(len(relations), 1),
            "top_connections": [
                {"user": r.to_user, "strength": r.strength, "frequency": r.frequency}
                for r in sorted(relations, key=lambda x: x.strength, reverse=True)[:5]
            ]
        }
        
        return behavior_stats

    def _summarize_communication_style(self, profile: UserProfile) -> Dict[str, str]:
        """æ€»ç»“æ²Ÿé€šé£æ ¼"""
        style_summary = {}
        
        if 'length' in profile.communication_style and profile.communication_style['length']:
            avg_length = sum(profile.communication_style['length']) / len(profile.communication_style['length'])
            if avg_length > 50:
                style_summary['length_style'] = 'è¯¦ç»†å‹'
            elif avg_length > 20:
                style_summary['length_style'] = 'é€‚ä¸­å‹'
            else:
                style_summary['length_style'] = 'ç®€æ´å‹'
        
        return style_summary

    def _summarize_activity_pattern(self, profile: UserProfile) -> Dict[str, Any]:
        """æ€»ç»“æ´»åŠ¨æ¨¡å¼"""
        activity_summary = {}
        
        if 'activity_hours' in profile.activity_pattern:
            hours = profile.activity_pattern['activity_hours']
            if hours:
                peak_hour = max(hours.items(), key=lambda x: x[1])[0] # ä¿®æ­£ä¸ºè·å–é”®
                activity_summary['peak_hour'] = peak_hour
                activity_summary['peak_period'] = self._get_time_period(peak_hour)
        
        return activity_summary

    async def export_social_graph(self) -> Dict[str, Any]:
        """å¯¼å‡ºç¤¾äº¤å…³ç³»å›¾è°±"""
        graph_data = {
            'nodes': [],
            'edges': [],
            'statistics': {}
        }
        
        # å¯¼å‡ºèŠ‚ç‚¹ï¼ˆç”¨æˆ·ï¼‰
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç”¨æˆ·ç”»åƒï¼Œè€Œä¸æ˜¯åªï¿½ï¿½ï¿½å†…å­˜ä¸­è·å–
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶ä½¿ç”¨å†…å­˜ä¸­çš„ user_profilesï¼Œä½†å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½
        for qq_id, profile in self.user_profiles.items():
            graph_data['nodes'].append({
                'id': qq_id,
                'name': profile.qq_name,
                'nicknames': profile.nicknames,
                'activity_level': len(profile.activity_pattern.get('activity_hours', {}))
            })
        
        # å¯¼å‡ºè¾¹ï¼ˆå…³ç³»ï¼‰
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç¤¾äº¤å…³ç³»ï¼Œè€Œä¸æ˜¯åªä»å†…å­˜ä¸­è·å–
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶ä½¿ç”¨å†…å­˜ä¸­çš„ social_graphï¼Œä½†å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½
        for from_user, relations in self.social_graph.items():
            for relation in relations:
                graph_data['edges'].append({
                    'from': from_user,
                    'to': relation.to_user,
                    'type': relation.relation_type,
                    'strength': relation.strength,
                    'frequency': relation.frequency
                })
        
        # ç»Ÿè®¡ä¿¡æ¯
        graph_data['statistics'] = {
            'total_users': len(self.user_profiles),
            'total_relations': sum(len(relations) for relations in self.social_graph.values()),
            'nickname_mappings': len(self.nickname_mapping)
        }
        
        return graph_data
    
    async def stop(self):
        """åœæ­¢å¤šç»´åº¦åˆ†æå™¨æœåŠ¡"""
        try:
            # å–æ¶ˆå®šæœŸæ¸…ç†ä»»åŠ¡
            if hasattr(self, '_cleanup_task') and self._cleanup_task:
                self._cleanup_task.cancel()
                try:
                    await self._cleanup_task
                except asyncio.CancelledError:
                    pass
                self._cleanup_task = None
            
            # ä¿å­˜é‡è¦çš„ç”¨æˆ·ç”»åƒæ•°æ®åˆ°æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦æŒä¹…åŒ–ï¼‰
            try:
                await self._save_user_profiles_to_db()
            except Exception as e:
                logger.warning(f"ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            
            # æ¸…ç†å†…å­˜æ•°æ®
            if hasattr(self, 'user_profiles'):
                self.user_profiles.clear()
            if hasattr(self, 'social_graph'):
                self.social_graph.clear()
            if hasattr(self, '_analysis_cache'):
                self._analysis_cache.clear()
            if hasattr(self, 'nickname_mapping'):
                self.nickname_mapping.clear()
            
            logger.info("å¤šç»´åº¦åˆ†æå™¨å·²åœæ­¢")
            return True
            
        except Exception as e:
            logger.error(f"åœæ­¢å¤šç»´åº¦åˆ†æå™¨å¤±è´¥: {e}")
            return False
    
    async def _save_user_profiles_to_db(self):
        """ä¿å­˜ç”¨æˆ·ç”»åƒæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if not self.user_profiles:
                return
                
            # è¿™é‡Œå¯ä»¥å®ç°å°†ç”¨æˆ·ç”»åƒæ•°æ®ä¿å­˜åˆ°ä¸“é—¨çš„ç”¨æˆ·ç”»åƒè¡¨
            # å½“å‰ç®€åŒ–å®ç°ï¼Œä»…è®°å½•ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"éœ€è¦ä¿å­˜ {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“")
            
            # TODO: å®ç°å…·ä½“çš„æ•°æ®åº“ä¿å­˜é€»è¾‘
            # ä¾‹å¦‚ï¼šCREATE TABLE user_profiles (group_id, user_id, profile_data, updated_at)
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“å¤±è´¥: {e}")
